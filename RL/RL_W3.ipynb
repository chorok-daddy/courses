{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPAZI2xqvO3K/Q7mEEZ3RDW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 3주차 Deep Q Network (DQN)\n","\n","\n"],"metadata":{"id":"5dBxklWwScDW"}},{"cell_type":"markdown","source":["## [핵심 목표]\n","\n","* DQN(Deep Q-Network) 알고리즘의 핵심 구성 요소를 이해하고 구현\n","* 경험 리플레이(Replay Buffer), ε-탐욕적(epsilon-greedy) 정책, **타겟 네트워크(Target Network)**의 개념을 이해\n","\n"],"metadata":{"id":"krZ1pT6_S1Li"}},{"cell_type":"markdown","source":["## [DQN 알고리즘 개요]\n","\n","* DQN은 Q-러닝과 신경망을 결합하여 복잡한 환경에서 강화학습을 수행하는 알고리즘\n","* Q-네트워크를 사용하여 Q-함수를 근사\n","\n","\\begin{array}{c}\n","\\boxed{\\text{Input: State } (s)} \\\\\n","\\downarrow \\\\\n","\\boxed{\\text{Layer 1: Fully Connected (Linear), } \\phi = W_1 s + b_1} \\\\\n","\\downarrow \\\\\n","\\boxed{\\text{Activation Function: ReLU, } a = \\max(0, \\phi)} \\\\\n","\\downarrow \\\\\n","\\boxed{\\text{Layer 2: Fully Connected (Linear), } v = W_2 a + b_2} \\\\\n","\\downarrow \\\\\n","\\boxed{\\text{Output: State Value } (V(s))}\n","\\end{array}\n","\n","* DQN의 주요 구성 요소:\n","    * 경험 리플레이 (Replay Buffer)\n","    * ε-탐욕적 (Epsilon-Greedy) 정책\n","    * 타겟 네트워크 (Target Network)"],"metadata":{"id":"WFhzYjP2S33G"}},{"cell_type":"markdown","source":["## [경험 리플레이 (Replay Buffer)]\n","\n","* 과거 경험을 메모리에 저장하고 무작위로 샘플링하여 학습 데이터 간의 상관 관계를 줄여 학습 안정성을 향상\n","* Transition: $(s_t, a_t, r_t, s_{t+1}, done)$ 형태의 경험을 저장\n","    * $s_t$: 현재 상태\n","    * $a_t$: 행동\n","    * $r_t$: 보상\n","    * $s_{t+1}$: 다음 상태\n","    * $done$: 에피소드 종료 여부\n","* 샘플링: 저장된 경험들로부터 무작위로 미니배치를 구성하여 학습에 사용\n","\n","## [ε-탐욕적 (Epsilon-Greedy) 정책]\n","\n","* **탐험(Exploration)**과 **활용(Exploitation)**의 균형을 맞춤\n","* ε 확률로 무작위 행동을 선택하고, 1 - ε 확률로 현재 Q-값이 가장 높은 행동을 선택\n","* ε 값은 학습이 진행됨에 따라 점차 감소시켜 탐험을 줄이고 활용을 늘림"],"metadata":{"id":"6oaR2q9xS_Vt"}},{"cell_type":"markdown","source":["## [타겟 네트워크 (Target Network)]\n","\n","* Q-값 업데이트의 안정성을 향상\n","* 메인 네트워크와 타겟 네트워크를 분리하여 사용\n","    * 메인 네트워크: Q값을 예측하고 업데이트하는 데 사용\n","    * 타겟 네트워크: 일정 주기마다 메인 네트워크의 파라미터를 복사하여 Q값을 계산하는 데 사용\n","* 타겟 네트워크를 고정함으로써 TD-오차의 변동성을 줄여 학습을 안정화\n","\n"],"metadata":{"id":"4d3e9aEPTEeI"}},{"cell_type":"markdown","source":["## [DQN 학습 업데이트]\n","\n","* 손실 함수 (Loss Function): TD (Temporal Difference) 오차를 최소화하는 방향으로 학습\n","* TD 타겟 (TD Target): $r + \\gamma \\max_{a'} Q(s', a'; \\theta^-)$\n","    * $r$: 즉각적인 보상\n","    * $\\gamma$: 할인율\n","    * $s'$: 다음 상태\n","    * $a'$: 다음 상태에서 타겟 네트워크를 사용하여 선택된 최적 행동\n","    * $\\theta^-$: 타겟 네트워크의 파라미터\n","* TD 오차 (TD Error): $TD ; Error = Q(s, a; \\theta) - (r + \\gamma \\max_{a'} Q(s', a'; \\theta^-))$\n","    * $\\theta$: 메인 네트워크의 파라미터\n","* 손실 함수: $Loss = \\mathbb{E}[(TD ; Error)^2]$\n","* 업데이트: 손실 함수를 최소화하는 방향으로 메인 네트워크의 파라미터를 업데이트\n","\n"],"metadata":{"id":"5556KymPTG9r"}},{"cell_type":"markdown","source":["## [ε-탐욕적 정책 액션 선택]\n","\n","* $\\epsilon$의 확률로 무작위 행동 선택\n","* $1 - \\epsilon$의 확률로 현재 Q-값이 최대인 행동 선택\n","\n"],"metadata":{"id":"aXi7pWDXTvJZ"}},{"cell_type":"markdown","source":["## [실습: DQN (Deep Q-Network) 구현 (CartPole)]"],"metadata":{"id":"K0OsIkScVe4A"}},{"cell_type":"markdown","source":["### 1. 환경 설정\n","* `gym`: CartPole 환경 제공\n","* `torch`: 딥러닝 모델 및 연산 처리\n","* `numpy`: 배열 및 수치 연산\n","* `collections.deque`: 경험 리플레이 버퍼 구현"],"metadata":{"id":"3mvJiCpjVpy3"}},{"cell_type":"code","source":["import gym\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from collections import deque\n","\n","env = gym.make('CartPole-v1')\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n"],"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}],"execution_count":null,"metadata":{"id":"crwi23NmVe4A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740660936404,"user_tz":-540,"elapsed":4121,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}},"outputId":"8979d38d-154f-408e-d33f-bb62d67eb513"}},{"cell_type":"markdown","source":["### 2. DQN 모델 정의\n","* `DQNAgent` 클래스: 신경망 모델 정의\n","* 3개의 완전 연결 계층 (`nn.Linear`) 사용\n","* ReLU 활성화 함수 적용 (`torch.relu`)\n","* 입력: 상태, 출력: 각 행동의 Q값"],"metadata":{"id":"Ks82Ue4JVe4B"}},{"cell_type":"code","source":["class DQNAgent(nn.Module):\n","    def __init__(self, state_size, action_size):\n","        super(DQNAgent, self).__init__()\n","        self.fc1 = nn.Linear(state_size, 24)  # 입력층\n","        self.fc2 = nn.Linear(24, 24)  # 은닉층\n","        self.fc3 = nn.Linear(24, action_size)  # 출력층\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))  # 활성화 함수 적용 (ReLU)\n","        x = torch.relu(self.fc2(x))\n","        return self.fc3(x)  # Q-값 반환"],"outputs":[],"execution_count":null,"metadata":{"id":"1bEat2I1Ve4B"}},{"cell_type":"markdown","source":["### 3. 하이퍼파라미터 설정\n","* 학습률 (`learning_rate`), 할인율 (`gamma`), 탐험 확률 (`epsilon`) 등 설정\n","* 미니배치 크기 (`batch_size`), 메모리 크기 (`memory_size`) 설정\n","* 타겟 네트워크 업데이트 주기 (`target_update_frequency`) 설정"],"metadata":{"id":"TFzo4kibVe4B"}},{"cell_type":"code","source":["learning_rate = 0.001\n","gamma = 0.99  # 할인율\n","epsilon = 1.0  # 초기 탐험 확률\n","epsilon_min = 0.01\n","epsilon_decay = 0.999\n","batch_size = 64\n","memory_size = 10000\n","target_update_frequency = 10  # 타겟 네트워크 업데이트 주기"],"outputs":[],"execution_count":null,"metadata":{"id":"YeqKp9poVe4B"}},{"cell_type":"markdown","source":["### 4. DQN 에이전트 및 타겟 네트워크 생성\n","* `DQNAgent` 모델 인스턴스 생성 (agent, target_agent)\n","* 타겟 네트워크 초기화 (agent 네트워크의 가중치 복사)\n","* Adam optimizer 및 MSE loss function 설정"],"metadata":{"id":"GsOqHNG6Ve4B"}},{"cell_type":"code","source":["agent = DQNAgent(state_size, action_size)\n","target_agent = DQNAgent(state_size, action_size)\n","target_agent.load_state_dict(agent.state_dict()) # Q 네트워크값 복사\n","optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n","criterion = nn.MSELoss()"],"outputs":[],"execution_count":null,"metadata":{"id":"q29m9-g-Ve4B"}},{"cell_type":"markdown","source":["### 5. 경험 리플레이 버퍼 초기화\n","* `deque`를 사용하여 경험 리플레이 버퍼 (`memory`) 초기화"],"metadata":{"id":"bSiBChxQVe4C"}},{"cell_type":"code","source":["memory = deque(maxlen=memory_size)"],"outputs":[],"execution_count":null,"metadata":{"id":"zgHSpWEDVe4C"}},{"cell_type":"markdown","source":["### 6. 학습 함수\n","* 경험 리플레이 버퍼에서 미니배치 샘플링\n","* 상태, 행동, 보상, 다음 상태, 종료 여부 추출\n","* 메인 네트워크를 사용하여 행동을 선택하고, 타겟 네트워크를 사용하여 선택된 행동의 Q값을 평가\n","* 예측 Q값과 타겟 Q값 비교하여 손실 함수 계산\n","* 역전파 및 가중치 업데이트"],"metadata":{"id":"3j0gTWDwVe4C"}},{"cell_type":"code","source":["def train(batch_size):\n","  # 경험 리플레이 버퍼에서 미니배치 샘플링\n","  minibatch = random.sample(memory, batch_size)\n","\n","  # 미니배치로부터 상태, 행동, 보상, 다음 상태, 종료 여부 추출\n","  states = torch.tensor(np.array([transition[0] for transition in minibatch]), dtype=torch.float32)\n","  actions = torch.tensor(np.array([transition[1] for transition in minibatch]), dtype=torch.long)\n","  rewards = torch.tensor(np.array([transition[2] for transition in minibatch]), dtype=torch.float32)\n","  next_states = torch.tensor(np.array([transition[3] for transition in minibatch]), dtype=torch.float32)\n","  dones = torch.tensor(np.array([transition[4] for transition in minibatch]), dtype=torch.float32)\n","\n","\n","  # 타겟 Q값 계산 (Target Network 사용)\n","  target_q_values = target_agent(next_states).max(1)[0].detach() # 타겟 네트워크에서 최대 Q값 가져오기\n","  targets = rewards + (gamma * target_q_values * (1 - dones)) # done = 1 이면 다음 상태의 Q값 0\n","\n","  # 예측 Q값 계산\n","  predicted_q_values = agent(states).gather(1, actions.unsqueeze(1)) # 학습된 네트워크에서 계산된 Q값 가져오기\n","\n","  # 손실 함수 계산 및 역전파\n","  loss = criterion(predicted_q_values.squeeze(), targets)\n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()"],"outputs":[],"execution_count":null,"metadata":{"id":"20TyeCm6Ve4C"}},{"cell_type":"markdown","source":["### 6. 학습 루프\n","* `episodes`만큼 에피소드 반복\n","* 각 에피소드마다 환경 초기화 및 상태 설정\n","* ε-탐욕적 정책으로 행동 선택\n","* 환경에서 행동 실행 후 다음 상태, 보상, 종료 여부 얻음\n","* 경험 리플레이 버퍼에 경험 추가\n","* 버퍼 크기가 `batch_size` 이상이면 학습 함수 호출\n","* 타겟 네트워크 업데이트 및 탐험 확률 감소\n","* 에피소드 결과 출력"],"metadata":{"id":"KqDBNnRMVe4C"}},{"cell_type":"code","source":["# 학습 루프\n","episodes = 100\n","for e in range(episodes):\n","    state = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32)\n","    done = False\n","    total_reward = 0\n","\n","    while not done:\n","        # ε-탐욕적 정책으로 행동 선택\n","        if random.random() < epsilon:\n","            action = env.action_space.sample()\n","        else:\n","            action = agent(state).argmax().item()\n","        # 환경에서 행동 실행 후 다음 상태, 보상, 종료 여부 얻음\n","        next_state, reward, done, _ = env.step(action)\n","        next_state = torch.tensor(next_state, dtype=torch.float32)\n","\n","        total_reward += reward\n","\n","        memory.append((state, action, reward, next_state, done))\n","\n","        if len(memory) >= batch_size:\n","            train(batch_size)\n","\n","        state = next_state\n","\n","        # 타겟 네트워크 업데이트\n","        if e % target_update_frequency == 0:\n","            target_agent.load_state_dict(agent.state_dict())\n","\n","        # 탐험 확률 감소\n","        if epsilon > epsilon_min:\n","            epsilon *= epsilon_decay\n","\n","    print(f\"Episode {e + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1/100, Total Reward: 16.0, Epsilon: 0.984\n","Episode 2/100, Total Reward: 15.0, Epsilon: 0.969\n","Episode 3/100, Total Reward: 21.0, Epsilon: 0.949\n","Episode 4/100, Total Reward: 18.0, Epsilon: 0.932\n","Episode 5/100, Total Reward: 15.0, Epsilon: 0.918\n","Episode 6/100, Total Reward: 38.0, Epsilon: 0.884\n","Episode 7/100, Total Reward: 29.0, Epsilon: 0.859\n","Episode 8/100, Total Reward: 37.0, Epsilon: 0.828\n","Episode 9/100, Total Reward: 10.0, Epsilon: 0.819\n","Episode 10/100, Total Reward: 23.0, Epsilon: 0.801\n","Episode 11/100, Total Reward: 11.0, Epsilon: 0.792\n","Episode 12/100, Total Reward: 26.0, Epsilon: 0.772\n","Episode 13/100, Total Reward: 15.0, Epsilon: 0.760\n","Episode 14/100, Total Reward: 16.0, Epsilon: 0.748\n","Episode 15/100, Total Reward: 30.0, Epsilon: 0.726\n","Episode 16/100, Total Reward: 68.0, Epsilon: 0.678\n","Episode 17/100, Total Reward: 26.0, Epsilon: 0.661\n","Episode 18/100, Total Reward: 15.0, Epsilon: 0.651\n","Episode 19/100, Total Reward: 142.0, Epsilon: 0.565\n","Episode 20/100, Total Reward: 41.0, Epsilon: 0.542\n","Episode 21/100, Total Reward: 24.0, Epsilon: 0.529\n","Episode 22/100, Total Reward: 14.0, Epsilon: 0.522\n","Episode 23/100, Total Reward: 258.0, Epsilon: 0.403\n","Episode 24/100, Total Reward: 178.0, Epsilon: 0.337\n","Episode 25/100, Total Reward: 196.0, Epsilon: 0.277\n","Episode 26/100, Total Reward: 183.0, Epsilon: 0.231\n","Episode 27/100, Total Reward: 193.0, Epsilon: 0.190\n","Episode 28/100, Total Reward: 191.0, Epsilon: 0.157\n","Episode 29/100, Total Reward: 188.0, Epsilon: 0.130\n","Episode 30/100, Total Reward: 181.0, Epsilon: 0.109\n","Episode 31/100, Total Reward: 500.0, Epsilon: 0.066\n","Episode 32/100, Total Reward: 500.0, Epsilon: 0.040\n","Episode 33/100, Total Reward: 500.0, Epsilon: 0.024\n","Episode 34/100, Total Reward: 500.0, Epsilon: 0.015\n","Episode 35/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 36/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 37/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 38/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 39/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 40/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 41/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 42/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 43/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 44/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 45/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 46/100, Total Reward: 190.0, Epsilon: 0.010\n","Episode 47/100, Total Reward: 166.0, Epsilon: 0.010\n","Episode 48/100, Total Reward: 152.0, Epsilon: 0.010\n","Episode 49/100, Total Reward: 153.0, Epsilon: 0.010\n","Episode 50/100, Total Reward: 152.0, Epsilon: 0.010\n","Episode 51/100, Total Reward: 150.0, Epsilon: 0.010\n","Episode 52/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 53/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 54/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 55/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 56/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 57/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 58/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 59/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 60/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 61/100, Total Reward: 331.0, Epsilon: 0.010\n","Episode 62/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 63/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 64/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 65/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 66/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 67/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 68/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 69/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 70/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 71/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 72/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 73/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 74/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 75/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 76/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 77/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 78/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 79/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 80/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 81/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 82/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 83/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 84/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 85/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 86/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 87/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 88/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 89/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 90/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 91/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 92/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 93/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 94/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 95/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 96/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 97/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 98/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 99/100, Total Reward: 162.0, Epsilon: 0.010\n","Episode 100/100, Total Reward: 131.0, Epsilon: 0.010\n"]}],"execution_count":null,"metadata":{"id":"QuUc57mkVe4C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740669740363,"user_tz":-540,"elapsed":91067,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}},"outputId":"315b82a0-9bc9-4d9a-9798-70bdd3a4a33f"}},{"cell_type":"markdown","source":["### 7. 학습된 에이전트 검증 (렌더링 with GIF)"],"metadata":{"id":"AkHIhwtpV3Ng"}},{"cell_type":"code","source":["import imageio\n","import os\n","\n","frames = []\n","state = env.reset()\n","state = torch.tensor(state, dtype=torch.float32)\n","done = False\n","for i in range(3000): # 3000 steps simulation\n","  action = agent(state).argmax().item()\n","  next_state, reward, done, _ = env.step(action)\n","  next_state = torch.tensor(next_state, dtype=torch.float32)\n","  state = next_state\n","\n","  if i % 10 == 0: # Render every 10 steps\n","    frames.append(env.render(mode='rgb_array'))\n","\n","  if done:\n","    break\n","\n","imageio.mimsave('cartpole_simulation.gif', frames, duration=33) # save to gif file\n","print(\"GIF saved as cartpole_simulation.gif\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fQdsiRmOevyM","executionInfo":{"status":"ok","timestamp":1740669174011,"user_tz":-540,"elapsed":1868,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}},"outputId":"a2e910ae-7065-486f-c776-cda05ad616ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GIF saved as cartpole_simulation.gif\n"]}]}]}