{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO4QZSoDTny6CtRqgAZmP5U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 3주차 Deep Q Network (DQN)\n","\n","\n"],"metadata":{"id":"5dBxklWwScDW"}},{"cell_type":"markdown","source":["## [핵심 목표]\n","\n","* DQN(Deep Q-Network) 알고리즘의 핵심 구성 요소를 이해하고 구현\n","* 경험 리플레이(Replay Buffer), ε-탐욕적(epsilon-greedy) 정책, **타겟 네트워크(Target Network)**의 개념을 이해\n","\n"],"metadata":{"id":"krZ1pT6_S1Li"}},{"cell_type":"markdown","source":["## [DQN 알고리즘 개요]\n","\n","* DQN은 Q-러닝과 신경망을 결합하여 복잡한 환경에서 강화학습을 수행하는 알고리즘\n","* Q-네트워크를 사용하여 Q-함수를 근사\n","\n","\\begin{array}{c}\n","\\boxed{\\text{Input: State } (s)} \\\\\n","\\downarrow \\\\\n","\\boxed{\\text{Layer 1: Fully Connected (Linear), } \\phi = W_1 s + b_1} \\\\\n","\\downarrow \\\\\n","\\boxed{\\text{Activation Function: ReLU, } a = \\max(0, \\phi)} \\\\\n","\\downarrow \\\\\n","\\boxed{\\text{Layer 2: Fully Connected (Linear), } v = W_2 a + b_2} \\\\\n","\\downarrow \\\\\n","\\boxed{\\text{Output: State Value } (V(s))}\n","\\end{array}\n","\n","* DQN의 주요 구성 요소:\n","    * 경험 리플레이 (Replay Buffer)\n","    * ε-탐욕적 (Epsilon-Greedy) 정책\n","    * 타겟 네트워크 (Target Network)"],"metadata":{"id":"WFhzYjP2S33G"}},{"cell_type":"markdown","source":["## [경험 리플레이 (Replay Buffer)]\n","\n","* 과거 경험을 메모리에 저장하고 무작위로 샘플링하여 학습 데이터 간의 상관 관계를 줄여 학습 안정성을 향상\n","* Transition: $(s_t, a_t, r_t, s_{t+1}, done)$ 형태의 경험을 저장\n","    * $s_t$: 현재 상태\n","    * $a_t$: 행동\n","    * $r_t$: 보상\n","    * $s_{t+1}$: 다음 상태\n","    * $done$: 에피소드 종료 여부\n","* 샘플링: 저장된 경험들로부터 무작위로 미니배치를 구성하여 학습에 사용\n","\n","## [ε-탐욕적 (Epsilon-Greedy) 정책]\n","\n","* **탐험(Exploration)**과 **활용(Exploitation)**의 균형을 맞춤\n","* ε 확률로 무작위 행동을 선택하고, 1 - ε 확률로 현재 Q-값이 가장 높은 행동을 선택\n","* ε 값은 학습이 진행됨에 따라 점차 감소시켜 탐험을 줄이고 활용을 늘림"],"metadata":{"id":"6oaR2q9xS_Vt"}},{"cell_type":"markdown","source":["## [타겟 네트워크 (Target Network)]\n","\n","* Q-값 업데이트의 안정성을 향상\n","* 메인 네트워크와 타겟 네트워크를 분리하여 사용\n","    * 메인 네트워크: Q값을 예측하고 업데이트하는 데 사용\n","    * 타겟 네트워크: 일정 주기마다 메인 네트워크의 파라미터를 복사하여 Q값을 계산하는 데 사용\n","* 타겟 네트워크를 고정함으로써 TD-오차의 변동성을 줄여 학습을 안정화\n","\n"],"metadata":{"id":"4d3e9aEPTEeI"}},{"cell_type":"markdown","source":["## [DQN 학습 업데이트]\n","\n","* 손실 함수 (Loss Function): TD (Temporal Difference) 오차를 최소화하는 방향으로 학습\n","* TD 타겟 (TD Target): $r + \\gamma \\max_{a'} Q(s', a'; \\theta^-)$\n","    * $r$: 즉각적인 보상\n","    * $\\gamma$: 할인율\n","    * $s'$: 다음 상태\n","    * $a'$: 다음 상태에서 타겟 네트워크를 사용하여 선택된 최적 행동\n","    * $\\theta^-$: 타겟 네트워크의 파라미터\n","* TD 오차 (TD Error): $TD ; Error = Q(s, a; \\theta) - (r + \\gamma \\max_{a'} Q(s', a'; \\theta^-))$\n","    * $\\theta$: 메인 네트워크의 파라미터\n","* 손실 함수: $Loss = \\mathbb{E}[(TD ; Error)^2]$\n","* 업데이트: 손실 함수를 최소화하는 방향으로 메인 네트워크의 파라미터를 업데이트\n","\n"],"metadata":{"id":"5556KymPTG9r"}},{"cell_type":"markdown","source":["## [ε-탐욕적 정책 액션 선택]\n","\n","* $\\epsilon$의 확률로 무작위 행동 선택\n","* $1 - \\epsilon$의 확률로 현재 Q-값이 최대인 행동 선택\n","\n"],"metadata":{"id":"aXi7pWDXTvJZ"}},{"cell_type":"markdown","source":["## [실습: DQN (Deep Q-Network) 구현 (CartPole)]"],"metadata":{"id":"K0OsIkScVe4A"}},{"cell_type":"markdown","source":["### 1. 환경 설정\n","* `gym`: CartPole 환경 제공\n","* `torch`: 딥러닝 모델 및 연산 처리\n","* `numpy`: 배열 및 수치 연산\n","* `collections.deque`: 경험 리플레이 버퍼 구현"],"metadata":{"id":"3mvJiCpjVpy3"}},{"cell_type":"code","source":["import gymnasium as gym\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from collections import deque\n","\n","env = gym.make('CartPole-v1', render_mode='rgb_array')\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n"],"outputs":[],"execution_count":45,"metadata":{"id":"crwi23NmVe4A","executionInfo":{"status":"ok","timestamp":1741085589736,"user_tz":-540,"elapsed":4,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 2. DQN 모델 정의\n","* `DQNAgent` 클래스: 신경망 모델 정의\n","* 3개의 완전 연결 계층 (`nn.Linear`) 사용\n","* ReLU 활성화 함수 적용 (`torch.relu`)\n","* 입력: 상태, 출력: 각 행동의 Q값"],"metadata":{"id":"Ks82Ue4JVe4B"}},{"cell_type":"code","source":["class DQNAgent(nn.Module):\n","    def __init__(self, state_size, action_size):\n","        super(DQNAgent, self).__init__()\n","        self.fc1 = nn.Linear(state_size, 128)  # 입력층\n","        self.fc2 = nn.Linear(128, 128)  # 은닉층\n","        self.fc3 = nn.Linear(128, action_size)  # 출력층\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))  # 활성화 함수 적용 (ReLU)\n","        x = torch.relu(self.fc2(x))\n","        return self.fc3(x)  # Q-값 반환"],"outputs":[],"execution_count":46,"metadata":{"id":"1bEat2I1Ve4B","executionInfo":{"status":"ok","timestamp":1741085590968,"user_tz":-540,"elapsed":2,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 3. 하이퍼파라미터 설정\n","* 학습률 (`learning_rate`), 할인율 (`gamma`), 탐험 확률 (`epsilon`) 등 설정\n","* 미니배치 크기 (`batch_size`), 메모리 크기 (`memory_size`) 설정\n","* 타겟 네트워크 업데이트 주기 (`target_update_frequency`) 설정"],"metadata":{"id":"TFzo4kibVe4B"}},{"cell_type":"code","source":["learning_rate = 0.0003\n","gamma = 0.99  # 할인율\n","epsilon = 1.0  # 초기 탐험 확률\n","epsilon_min = 0.01\n","epsilon_decay = 0.995\n","batch_size = 128\n","memory_size = 30000\n","target_update_frequency = 5  # 타겟 네트워크 업데이트 주기"],"outputs":[],"execution_count":47,"metadata":{"id":"YeqKp9poVe4B","executionInfo":{"status":"ok","timestamp":1741085592584,"user_tz":-540,"elapsed":10,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 4. DQN 에이전트 및 타겟 네트워크 생성\n","* `DQNAgent` 모델 인스턴스 생성 (agent, target_agent)\n","* 타겟 네트워크 초기화 (agent 네트워크의 가중치 복사)\n","* Adam optimizer 및 MSE loss function 설정"],"metadata":{"id":"GsOqHNG6Ve4B"}},{"cell_type":"code","source":["agent = DQNAgent(state_size, action_size)\n","target_agent = DQNAgent(state_size, action_size)\n","target_agent.load_state_dict(agent.state_dict()) # Q 네트워크값 복사\n","optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n","criterion = nn.MSELoss()"],"outputs":[],"execution_count":48,"metadata":{"id":"q29m9-g-Ve4B","executionInfo":{"status":"ok","timestamp":1741085594609,"user_tz":-540,"elapsed":9,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 5. 경험 리플레이 버퍼 초기화\n","* `deque`를 사용하여 경험 리플레이 버퍼 (`memory`) 초기화"],"metadata":{"id":"bSiBChxQVe4C"}},{"cell_type":"code","source":["memory = deque(maxlen=memory_size)"],"outputs":[],"execution_count":49,"metadata":{"id":"zgHSpWEDVe4C","executionInfo":{"status":"ok","timestamp":1741085596321,"user_tz":-540,"elapsed":4,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 6. 학습 함수\n","* 경험 리플레이 버퍼에서 미니배치 샘플링\n","* 상태, 행동, 보상, 다음 상태, 종료 여부 추출\n","* 메인 네트워크를 사용하여 행동을 선택하고, 타겟 네트워크를 사용하여 선택된 행동의 Q값을 평가\n","* 예측 Q값과 타겟 Q값 비교하여 손실 함수 계산\n","* 역전파 및 가중치 업데이트"],"metadata":{"id":"3j0gTWDwVe4C"}},{"cell_type":"code","source":["def train(batch_size):\n","  # 경험 리플레이 버퍼에서 미니배치 샘플링\n","  minibatch = random.sample(memory, batch_size)\n","\n","  # 미니배치로부터 상태, 행동, 보상, 다음 상태, 종료 여부 추출\n","  states = torch.tensor(np.array([transition[0] for transition in minibatch]), dtype=torch.float32)\n","  actions = torch.tensor(np.array([transition[1] for transition in minibatch]), dtype=torch.long)\n","  rewards = torch.tensor(np.array([transition[2] for transition in minibatch]), dtype=torch.float32)\n","  next_states = torch.tensor(np.array([transition[3] for transition in minibatch]), dtype=torch.float32)\n","  dones = torch.tensor(np.array([transition[4] for transition in minibatch]), dtype=torch.float32)\n","\n","  # 타겟 Q값 계산 (Target Network 사용)\n","  target_q_values = target_agent(next_states).max(1)[0].detach() # 타겟 네트워크에서 최대 Q값 가져오기\n","  targets = rewards + (gamma * target_q_values * (1 - dones)) # done = 1 이면 다음 상태의 Q값 0\n","\n","  # 예측 Q값 계산\n","  predicted_q_values = agent(states).gather(1, actions.unsqueeze(1)) # 학습된 네트워크에서 계산된 Q값 가져오기\n","\n","  # 손실 함수 계산 및 역전파\n","  loss = criterion(predicted_q_values.squeeze(), targets)\n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()"],"outputs":[],"execution_count":50,"metadata":{"id":"20TyeCm6Ve4C","executionInfo":{"status":"ok","timestamp":1741085601591,"user_tz":-540,"elapsed":42,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 6. 학습 루프\n","* `episodes`만큼 에피소드 반복\n","* 각 에피소드마다 환경 초기화 및 상태 설정\n","* ε-탐욕적 정책으로 행동 선택\n","* 환경에서 행동 실행 후 다음 상태, 보상, 종료 여부 얻음\n","* 경험 리플레이 버퍼에 경험 추가\n","* 버퍼 크기가 `batch_size` 이상이면 학습 함수 호출\n","* 타겟 네트워크 업데이트 및 탐험 확률 감소\n","* 에피소드 결과 출력"],"metadata":{"id":"KqDBNnRMVe4C"}},{"cell_type":"code","source":["# 학습 루프\n","episodes = 100\n","for e in range(episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32)\n","    done = False\n","    total_reward = 0\n","\n","    while not done:\n","        # ε-탐욕적 정책으로 행동 선택\n","        if random.random() < epsilon:\n","            action = env.action_space.sample()\n","        else:\n","            action = agent(state).argmax().item()\n","        # 환경에서 행동 실행 후 다음 상태, 보상, 종료 여부 얻음\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        next_state = torch.tensor(next_state, dtype=torch.float32)\n","        total_reward += reward\n","        done = terminated or truncated\n","\n","        memory.append((state, action, reward, next_state, done))\n","\n","        if len(memory) >= batch_size:\n","            train(batch_size)\n","\n","        state = next_state\n","\n","        # 타겟 네트워크 업데이트\n","        if e % target_update_frequency == 0:\n","            target_agent.load_state_dict(agent.state_dict())\n","\n","        # 탐험 확률 감소\n","        if epsilon > epsilon_min:\n","            epsilon *= epsilon_decay\n","\n","    # 중간 결과 출력\n","    if (e+1) % 20 == 0:\n","        print(f\"Episode {e + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 20/100, Total Reward: 11.0, Epsilon: 0.220\n","Episode 40/100, Total Reward: 24.0, Epsilon: 0.067\n","Episode 60/100, Total Reward: 118.0, Epsilon: 0.010\n","Episode 80/100, Total Reward: 158.0, Epsilon: 0.010\n","Episode 100/100, Total Reward: 199.0, Epsilon: 0.010\n"]}],"execution_count":51,"metadata":{"id":"QuUc57mkVe4C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741085661883,"user_tz":-540,"elapsed":56381,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}},"outputId":"38383697-258b-45e9-ac90-964d0f64486c"}},{"cell_type":"markdown","source":["### 7. 학습된 에이전트 검증 (렌더링 with GIF)"],"metadata":{"id":"AkHIhwtpV3Ng"}},{"cell_type":"code","source":["import imageio\n","import os\n","\n","frames = []\n","state, _ = env.reset()\n","state = torch.tensor(state, dtype=torch.float32)\n","done = False\n","for i in range(3000): # 3000 steps simulation\n","  action = agent(state).argmax().item()\n","  next_state, reward, terminated, truncated, _ = env.step(action)\n","  next_state = torch.tensor(next_state, dtype=torch.float32)\n","  state = next_state\n","  done = terminated or truncated\n","  frames.append(env.render())\n","\n","  if done:\n","    break\n","\n","imageio.mimsave('cartpole_simulation.gif', frames, duration=33) # save to gif file\n","print(\"GIF saved as cartpole_simulation.gif\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fQdsiRmOevyM","executionInfo":{"status":"ok","timestamp":1741085692846,"user_tz":-540,"elapsed":10352,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}},"outputId":"b3918b24-febb-4443-c87d-597fd2563c89"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["GIF saved as cartpole_simulation.gif\n"]}]}]}