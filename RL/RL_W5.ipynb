{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOVmLK38fXIs167e5Dryasw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 5주차 Dueling DQN\n"],"metadata":{"id":"Z_ufd5QXNUjW"}},{"cell_type":"markdown","source":["## [핵심 목표]\n","\n","* Dueling DQN 개념 이해 및 Q-함수 구조 개선으로 학습 효율 증대\n","* Value Stream과 Advantage Stream 역할 이해 및 Dueling DQN 구현"],"metadata":{"id":"nX6uOPIRO3R3"}},{"cell_type":"markdown","source":["## [Dueling DQN 아이디어]\n","\n","* Q함수를 Value Stream과 Advantage Stream으로 분리\n","    * **Value Stream ($V(s)$)**: 상태 가치 예측, 상태 $s$의 좋음/나쁨 정도\n","    * **Advantage Stream ($A(s,a)$)**: 각 행동 Advantage 예측, 상태 $s$에서 행동 $a$ 선택 시 **평균 행동 대비** 얼마나 좋은지\n","* Q함수: Value Stream과 Advantage Stream 결합, Q값 계산\n","* 수식: $Q(s,a) = V(s) + A(s,a)$"],"metadata":{"id":"2lzrhLFgO7CU"}},{"cell_type":"markdown","source":["## [Dueling DQN 장점]\n","\n","* 상태 가치와 행동 Advantage 분리 학습, Q값 추정 정확도 향상\n","* **Value Stream**: 상태에 대한 일반 정보 학습, 다양한 행동에 대한 Q값 효율적 추정\n","* **Advantage Stream**: 특정 행동이 다른 행동보다 나은 정도 학습, 행동 간 미세 차이 구별 용이"],"metadata":{"id":"W-Q51defO97K"}},{"cell_type":"markdown","source":["## [Dueling DQN 네트워크]\n","* 네트워크 구조\n","\\begin{array}{c}\n","\\boxed{\\text{State }(s)} \\\\\n","\\downarrow \\\\\n","\\boxed{\\text{Feature Extraction }} \\\\\n","\\swarrow \\quad \\searrow \\quad \\quad \\quad \\\\\n","\\boxed{\\begin{array}{c} \\text{Value Stream} \\\\ \\downarrow \\\\ V(s) \\end{array}} \\quad\n","\\boxed{\\begin{array}{c} \\text{Advantage Stream} \\\\ \\downarrow \\\\ A(s, a_1), \\dots, A(s, a_N) \\end{array}} \\\\\n","\\searrow \\quad \\swarrow \\quad \\quad \\quad \\\\\n","\\boxed{\\text{Aggregation: } Q(s, a) = V(s) + \\left(A(s, a) - \\frac{1}{|\\mathbb{A}|}\\sum_{a'} A(s, a')\\right)} \\\\\n","\\downarrow \\\\\n","\\boxed{Q(s, a_1), Q(s, a_2), \\dots, Q(s, a_N)}\n","\\end{array}\n","\n","* 구성 요소\n","\n","  1.  **State (s) (입력):**\n","      *   환경의 상태 정보 (예: 게임 화면, 센서 데이터)\n","\n","  2.  **Feature Extraction (Shared Layers):**\n","      *   **종류:** Convolutional Layers (`Conv`, 이미지), Fully Connected Layers (`FC`, 벡터)\n","      *   **역할:** 입력 상태 $s$ $\\rightarrow$ 유용한 특징 추출\n","\n","  3.  **Value Stream (Separate Layers):**\n","      *   **종류:** 주로 Fully Connected Layers (`FC`)\n","      *   **역할:** 상태 가치 ($V(s)$) 추정\n","      *   **출력:** 단일 값 (scalar)\n","\n","  4.  **Advantage Stream (Separate Layers):**\n","      *   **종류:** 주로 Fully Connected Layers (`FC`)\n","      *   **역할:** 각 행동의 이점 ($A(s, a)$) 추정\n","      *   **출력:** 행동 개수만큼의 값\n","\n","  5.  **Aggregation (결합):**\n","\n","      *   **수식:** $Q(s, a) = V(s) + \\left( A(s, a) - \\frac{1}{|\\mathbb{A}|}\\sum_{a'} A(s, a') \\right)$\n","        * 안정성 위해 평균 Advantage 빼줌\n","        * $|\\mathbb{A}|$: 가능한 행동의 개수\n","      *   **역할:** $V(s) + A(s, a) =$ 최종 Q-value\n"],"metadata":{"id":"uRX0Zx2bPALA"}},{"cell_type":"markdown","source":["## [Dueling DQN 학습 과정]\n"],"metadata":{"id":"Vv3uLO9NPDOl"}},{"cell_type":"markdown","source":["### 1. Q값 계산\n","\n","* 현재 Q값\n","  * 메인 네트워크($\\theta$)를 사용하여 현재 상태에 대한 Q값을 계산\n","  * $Q(s, a; \\theta)$\n","* 타겟 Q값\n","  * 타겟 네트워크($\\theta^-$)를 사용하여 다음 상태에서 최대 Q값을 계산\n","  * $y = r + \\gamma \\underset{a'}{max} \\text{ } Q(s', a'; \\theta^-)$\n","    * $\\gamma$: 할인율"],"metadata":{"id":"YS7AI5PVZO2S"}},{"cell_type":"markdown","source":["### 2. 손실 함수 계산\n","\n","* MSE 손실\n","  * 계산된 현재 Q값과 타겟 Q값 사이의 MSE 손실을 계산\n","  * $L(\\theta) = \\mathbb{E}[(Q(s, a; \\theta) - y)^2]$"],"metadata":{"id":"M3_JcmgaZ-AJ"}},{"cell_type":"markdown","source":["### 3. 네트워크 업데이트\n","\n","* 역전파\n","  * 계산된 손실 $L(\\theta)$을 사용하여 메인 네트워크의 파라미터 $\\theta$를 업데이트\n","  * $\\theta \\leftarrow \\theta - \\alpha \\nabla_{\\theta} L(\\theta)$\n","    * $\\alpha$: 학습률"],"metadata":{"id":"S2b2UfY3aMna"}},{"cell_type":"markdown","source":["### 4. 타겟 네트워크 업데이트\n","\n","* 일정 스텝(step)마다 메인 네트워크의 파라미터를 타겟 네트워크에 복사하여 타겟 네트워크 업데이트합니다.\n","* $\\theta^- \\leftarrow \\theta$\n"],"metadata":{"id":"pWvHwcc1aT8x"}},{"cell_type":"markdown","source":["## [실습: Dueling DQN을 이용한 CartPole 실습]\n"],"metadata":{"id":"CsKnBZvYrh73"}},{"cell_type":"markdown","source":["### 1. 라이브러리 설치 및 가져오기\n","* `gym`: CartPole 환경 제공\n","* `torch`: 딥러닝 모델 및 연산 처리\n","* `numpy`: 배열 및 수치 연산\n","* `collections.deque`: 경험 리플레이 버퍼 구현"],"metadata":{"id":"h3UdxOOSH7J_"}},{"cell_type":"code","source":["import gymnasium as gym\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import random\n","from collections import deque\n","import numpy as np\n","import imageio"],"outputs":[],"execution_count":1,"metadata":{"id":"Dtsm9_rfH7KC","executionInfo":{"status":"ok","timestamp":1741087121279,"user_tz":-540,"elapsed":6741,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 2. GPU 설정 및 하이퍼파라미터 정의\n","\n","*   **`BATCH_SIZE`**: 한 번 학습에 사용하는 샘플 수\n","*   **`GAMMA`**: 미래 보상 할인 정도 (0~1)\n","*   **`EPS_START`**: 초기 무작위 행동 확률\n","*   **`EPS_END`**: 최종 무작위 행동 확률\n","*   **`EPS_DECAY`**: 무작위 행동 확률 감소 속도\n","*   **`TARGET_UPDATE`**: Target Network 업데이트 간격\n","*   **`MEMORY_SIZE`**: Replay Buffer 저장 용량\n","*   **`LR`**: 가중치 업데이트 크기 (학습률)\n","*   **`NUM_EPISODES`**: 총 학습 에피소드 횟수\n","*   **`RENDER_EPISODE`**: 통계 출력 주기\n"],"metadata":{"id":"J3HE0bV3H7KD"}},{"cell_type":"code","source":["# GPU 사용 가능 여부 확인 및 장치 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 하이퍼파라미터\n","BATCH_SIZE = 128\n","GAMMA = 0.99\n","EPS_START = 1.0\n","EPS_END = 0.01\n","EPS_DECAY = 0.995\n","TARGET_UPDATE = 5\n","MEMORY_SIZE = 30000\n","LR = 0.0003\n","NUM_EPISODES = 2000\n","PROGRESS_INTERVAL = 50"],"outputs":[],"execution_count":2,"metadata":{"id":"KanK5-ScH7KD","executionInfo":{"status":"ok","timestamp":1741087123608,"user_tz":-540,"elapsed":2,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 3. Dueling DQN 네트워크 정의\n","\n","* **`DuelingDQN` 클래스**: PyTorch의 `nn.Module`을 상속받아 네트워크 구성.\n","* **`feature_layer`**: 입력 상태(state)를 처리하는 공통 특징 추출 레이어.\n","* **`value_stream`**: 상태의 가치(Value, V)를 추정하는 레이어.\n","* **`advantage_stream`**: 각 행동의 상대적 이점(Advantage, A)을 추정하는 레이어.\n","* **`forward` 메서드**: Value와 Advantage를 결합하여 최종 Q-value를 계산.\n","\n","* **(Q 값 계산)**:\n","$Q(s, a) = V(s) + \\left(A(s, a) - \\frac{1}{|\\mathbb{A}|}\\sum_{a'} A(s, a')\\right)$\n","  * $Q(s, a)$: 상태 $s$에서 행동 $a$를 선택했을 때의 Q-value\n","  * $V(s)$: 상태 $s$의 가치(Value)\n","  * $A(s, a)$: 상태 $s$에서 행동 $a$의 이점(Advantage)\n","  * $|\\mathbb{A}|$: 행동 공간의 크기 (가능한 행동의 개수)"],"metadata":{"id":"jGmyV1VzH7KE"}},{"cell_type":"code","source":["class DuelingDQN(nn.Module):\n","    def __init__(self, state_size, action_size, hidden_size):\n","        super(DuelingDQN, self).__init__()\n","        self.feature_layer = nn.Sequential(\n","            nn.Linear(state_size, hidden_size),\n","            nn.ReLU()\n","        )\n","        self.value_stream = nn.Sequential(\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, 1)\n","        )\n","        self.advantage_stream = nn.Sequential(\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, action_size)\n","        )\n","\n","    def forward(self, state):\n","        x = self.feature_layer(state)\n","        value = self.value_stream(x)\n","        advantage = self.advantage_stream(x)\n","        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n","        return q_values"],"outputs":[],"execution_count":3,"metadata":{"id":"mX8TFpUFH7KE","executionInfo":{"status":"ok","timestamp":1741087129154,"user_tz":-540,"elapsed":13,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 4. Replay Buffer 정의\n","\n","* **`ReplayBuffer` 클래스**: `deque`를 사용하여 고정된 크기의 버퍼를 생성\n","* **`push` 메서드**: 새로운 경험 (state, action, reward, next\\_state, done)을 버퍼에 추가\n","* **`sample` 메서드**: 버퍼에서 무작위로 `batch_size`만큼의 경험을 샘플링하여 PyTorch 텐서로 변환 (GPU 사용 고려)\n","* **`__len__` 메서드**: 버퍼에 저장된 경험의 개수를 반환"],"metadata":{"id":"dmlKBVMpH7KE"}},{"cell_type":"code","source":["class ReplayBuffer:\n","    def __init__(self, capacity):\n","        self.buffer = deque(maxlen=capacity)\n","\n","    def push(self, state, action, reward, next_state, done):\n","        state = np.array(state)\n","        next_state = np.array(next_state)\n","        self.buffer.append((state, action, reward, next_state, done))\n","\n","    def sample(self, batch_size):\n","        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n","        return torch.tensor(np.array(state), dtype=torch.float32, device=device), \\\n","           torch.tensor(np.array(action), dtype=torch.int64, device=device), \\\n","           torch.tensor(np.array(reward), dtype=torch.float32, device=device), \\\n","           torch.tensor(np.array(next_state), dtype=torch.float32, device=device), \\\n","           torch.tensor(np.array(done), dtype=torch.float32, device=device)\n","\n","    def __len__(self):\n","        return len(self.buffer)"],"outputs":[],"execution_count":4,"metadata":{"id":"RyYR7KrPH7KF","executionInfo":{"status":"ok","timestamp":1741087131120,"user_tz":-540,"elapsed":2,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 5. 학습 함수 정의\n","* **Epsilon-Greedy**: 탐험(Exploration)과 활용(Exploitation)의 균형을 맞추기 위해 사용\n","  * `epsilon` 값은 `EPS_START`에서 시작하여 `EPS_END`를 향해 지수적으로 감소(`EPS_DECAY` 사용)\n","  * 무작위 값이 `epsilon`보다 크면, 학습된 정책에 따라 행동 선택 (Exploitation)\n","  * 무작위 값이 `epsilon`보다 작거나 같으면, 무작위 행동 선택 (Exploration)\n","\n","* **Replay Buffer**:\n","  * `push`: 매 스텝마다 얻은 경험(transition)을 Replay Buffer에 저장\n","  * `sample`: Replay Buffer에서 미니배치 크기만큼의 경험을 무작위로 추출하여 학습에 사용\n","\n","* **Target Network**:\n","  * 학습의 안정성을 높이기 위해 사용\n","  * `TARGET_UPDATE` 에피소드마다 `policy_net`의 가중치를 `target_net`으로 복사\n","\n","* **Loss Function (MSE Loss)**:\n","  * TD Error(Temporal Difference Error)를 최소화하는 방향으로 학습\n","  * TD Target: $r + \\gamma \\max_{a'} Q_{\\text{target}}(s', a')$\n","  * Loss: $\\text{MSE}(Q(s, a), r + \\gamma \\max_{a'} Q_{\\text{target}}(s', a'))$\n","    * $Q(s, a)$: `policy_net`을 사용하여 예측한 Q-value\n","    * $r$: 현재 스텝에서 얻은 보상\n","    * $\\gamma$: 할인율 (Discount Factor)\n","    * $\\max_{a'} Q_{\\text{target}}(s', a')$: `target_net`을 사용하여 계산한 다음 상태($s'$)의 최대 Q-value"],"metadata":{"id":"xb2QaeWTH7KF"}},{"cell_type":"code","source":["def train(env, policy_net, target_net, optimizer, replay_buffer):\n","    all_rewards = []\n","\n","    for episode in range(NUM_EPISODES):\n","        state, _ = env.reset()\n","        total_reward = 0\n","        done = False\n","\n","        while not done:\n","            # Epsilon-Greedy Exploration\n","            epsilon = max(EPS_END, EPS_START * (EPS_DECAY ** episode))\n","            if random.random() > epsilon:\n","                with torch.no_grad():\n","                    state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n","                    q_values = policy_net(state_tensor)\n","                    action = q_values.argmax(dim=1).item()\n","            else:\n","                action = env.action_space.sample()\n","\n","            next_state, reward, terminated, truncated, _ = env.step(action)\n","            total_reward += reward\n","            done = terminated or truncated\n","\n","            # Replay Buffer에 경험 저장\n","            replay_buffer.push(state, action, reward, next_state, done)\n","            state = next_state\n","\n","            # Replay Buffer에서 샘플링 및 학습\n","            if len(replay_buffer) >= BATCH_SIZE:\n","                state_batch, action_batch, reward_batch, next_state_batch, done_batch = replay_buffer.sample(BATCH_SIZE)\n","\n","                # Double DQN Target Q-value 계산\n","                with torch.no_grad():\n","                    next_actions = policy_net(next_state_batch).argmax(dim=1, keepdim=True)\n","                    next_q_values = target_net(next_state_batch).gather(1, next_actions).squeeze(1)\n","                    target_q_values = reward_batch + (1 - done_batch) * GAMMA * next_q_values\n","\n","                # 현재 Q-value 계산\n","                q_values = policy_net(state_batch)\n","                q_values_for_actions = q_values.gather(1, action_batch.unsqueeze(1)).squeeze(1)\n","\n","                # Loss 계산 및 최적화\n","                loss = nn.functional.mse_loss(q_values_for_actions, target_q_values)\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","        all_rewards.append(total_reward)\n","\n","        # Target Network 업데이트\n","        if (episode+1) % TARGET_UPDATE == 0:\n","            target_net.load_state_dict(policy_net.state_dict())\n","\n","        # 평균 리워드 출력\n","        if (episode+1) % PROGRESS_INTERVAL == 0:\n","            print(f\"Episode {episode+1}: Average Reward over last {PROGRESS_INTERVAL} episodes = {np.mean(all_rewards[-PROGRESS_INTERVAL:])}\")\n","\n","        # 오랜 기간 리워드가 충분히 크다면 학습 종료\n","        if np.min(all_rewards[-TARGET_UPDATE*2:]) >= 450:\n","            print(f\"Solved in {episode+1} episodes with reward {total_reward}!\")\n","            break\n"],"outputs":[],"execution_count":5,"metadata":{"id":"z9e6G-t0H7KF","executionInfo":{"status":"ok","timestamp":1741087133283,"user_tz":-540,"elapsed":3,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 6. 학습 실행 및 결과 확인\n","\n","  * **`imageio`**: GIF 이미지 생성을 위한 라이브러리\n","  * 환경 초기화: `gym.make(\"CartPole-v1\")`로 CartPole 환경을 생성\n","  * 네트워크 생성: `policy_net` (학습용), `target_net` (안정성 향상)을 생성하고 GPU/CPU로 이동\n","  * 옵티마이저, Replay Buffer: `Adam` 옵티마이저와 `ReplayBuffer` 객체를 생성\n","  * `train` 함수 호출: 학습을 시작\n","  * **GIF 생성**: `imageio`를 사용하여 학습된 에이전트의 플레이를 GIF로 저장\n","  * `env.close()`: 환경을 종료\n"],"metadata":{"id":"tzkKZxh-H7KG"}},{"cell_type":"code","source":["env = gym.make(\"CartPole-v1\", render_mode = 'rgb_array')\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n\n","hidden_size = 128\n","\n","policy_net = DuelingDQN(state_size, action_size, hidden_size).to(device)\n","target_net = DuelingDQN(state_size, action_size, hidden_size).to(device)\n","target_net.load_state_dict(policy_net.state_dict())\n","target_net.eval()\n","\n","optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n","replay_buffer = ReplayBuffer(MEMORY_SIZE)\n","\n","train(env, policy_net, target_net, optimizer, replay_buffer)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 50: Average Reward over last 50 episodes = 27.6\n","Episode 100: Average Reward over last 50 episodes = 42.24\n","Episode 150: Average Reward over last 50 episodes = 93.7\n","Episode 200: Average Reward over last 50 episodes = 79.7\n","Episode 250: Average Reward over last 50 episodes = 116.52\n","Episode 300: Average Reward over last 50 episodes = 107.16\n","Episode 350: Average Reward over last 50 episodes = 128.58\n","Episode 400: Average Reward over last 50 episodes = 122.18\n","Episode 450: Average Reward over last 50 episodes = 185.16\n","Episode 500: Average Reward over last 50 episodes = 168.18\n","Episode 550: Average Reward over last 50 episodes = 291.82\n","Solved in 575 episodes with reward 500.0!\n"]}],"execution_count":6,"metadata":{"id":"FCt9tOwkH7KG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741087497744,"user_tz":-540,"elapsed":356648,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}},"outputId":"0a516143-a097-435c-d477-1c2fcc9a4d5d"}},{"cell_type":"code","source":["# 렌더링 및 GIF 저장\n","frames = []\n","state, _ = env.reset()\n","done = False\n","total_reward = 0\n","while not done:\n","    with torch.no_grad():\n","        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n","        q_values = policy_net(state_tensor)\n","        action = q_values.argmax(dim=1).item()\n","    next_state, reward, terminated, truncated, _ = env.step(action)\n","    total_reward += reward\n","    state = next_state\n","    done = terminated or truncated\n","    frames.append(env.render())\n","\n","imageio.mimsave('cartpole_simulation_dueling_double_DQN.gif', frames, duration=33)\n","print(f\"Final episode reward: {total_reward}\")\n","env.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4apBwgiputSk","executionInfo":{"status":"ok","timestamp":1741087532050,"user_tz":-540,"elapsed":18488,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}},"outputId":"b715b3ed-9e1d-4d1a-b915-4be3376b9e19"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Final episode reward: 399.0\n"]}]}]}