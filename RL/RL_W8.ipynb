{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN2g99YvSQ5XhSsHpKhqaDp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Advantage Actor-Critic (A2C)"],"metadata":{"id":"Ess-BeySSau_"}},{"cell_type":"markdown","source":["## [Actor-Critic 핵심 개념]\n","\n","* **정책 기반**과 **가치 기반** 방법을 결합\n","  * **Actor** (정책 결정): 정책 $\\pi_\\theta(a|s)$를 파라미터화, 상태에서 행동 확률 분포 출력\n","  * **Critic** (가치 평가): 가치 함수 $V_\\psi(s)$를 파라미터화, 현재 상태의 가치(기대 반환) 추정\n","* **장점**\n","  * **실시간 반응**: 에피소드가 끝나지 않아도 업데이트 가능\n","    * 에피소드 단위로 Return 계산하지 않음\n","    * 환경 변화에 즉각적 대응 가능 > REINFORCE의 **고분산** 문제 완화\n"],"metadata":{"id":"GiLNlicIQ8gI"}},{"cell_type":"markdown","source":["## [Advantage Actor-Critic (A2C) 알고리즘]\n","\n","* **Advantage 함수** 사용\n","  * $A(s, a) = Q(s, a) - V(s)$: 특정 상태에서 행동을 선택했을 때 얻는 추가적인 이득\n","  * **Critic**은 $V(s)$를 추정, **Actor**는 $A(s, a)$를 최대화하는 방향으로 학습\n","* **업데이트 방식**\n","  * **Actor**: $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a|s) A(s, a)$\n","    * 정책 경사(Policy Gradient)를 사용하여 정책 업데이트    \n","    * $\\alpha$: 학습률\n","  * **Critic**: $\\psi \\leftarrow \\psi + \\beta (r + \\gamma V(s') - V(s)) \\nabla_\\psi V(s)$\n","    * TD 오차(Temporal-Difference error)를 최소화하는 방향으로 가치 함수 업데이트\n","    * $\\beta$: 학습률\n","    * $r$: 보상\n","    * $\\gamma$: 할인율\n","    * $s'$: 다음 상태\n","*   **A2C의 효능**\n","    *   **안정적인 학습**: Critic이 가치를 평가, Actor는 Critic의 평가를 기반으로 정책을 업데이트하므로, REINFORCE에 비해 안정적인 학습 가능\n","    *   **효율적인 탐색**: Advantage 함수를 사용하여, 어떤 행동이 평균보다 더 나은 결과를 가져올 수 있는지 파악, 효율적인 탐색 가능"],"metadata":{"id":"pqk4NmpqSZVJ"}},{"cell_type":"markdown","source":["## [A2C 구현]\n","\n","* **Actor 네트워크**와 **Critic 네트워크** 설계\n","  * Actor 네트워크: $s \\rightarrow \\boxed{\\text{FC} + \\text{ReLU}} \\rightarrow \\boxed{\\text{FC} + \\text{Softmax}} \\rightarrow \\pi(a|s)$\n","    * 상태를 입력받아 행동에 대한 확률 분포 출력\n","    * **REINFORCE의 정책 네트워크 구조와 동일**\n","  * Critic 네크워크: $s \\rightarrow \\boxed{\\text{FC} + \\text{ReLU}} \\rightarrow \\boxed{\\text{FC}} \\rightarrow V(s)$\n","    * 상태를 입력받아 해당 상태의 가치(Value)를 출력\n","    * **DQN의 Q-네트워크 구조와 동일**\n","* **환경**과의 **상호작용**을 통해 **데이터** 수집\n","* 수집된 데이터를 사용하여 **Actor**와 **Critic** 네트워크 **업데이트**\n","\n","\n"],"metadata":{"id":"pGqA6KapT04m"}},{"cell_type":"markdown","source":["## [A2C 한계 및 개선 방향]\n","\n","* **On-policy** 학습: 현재 정책에 따라서만 학습 데이터를 수집하므로, 샘플 효율성이 낮음\n","* **개선 방향**\n","  * **Proximal Policy Optimization (PPO)** (Week 9)\n","    * 정책 업데이트 시, 새로운 정책과 기존 정책의 차이를 제한하여 안정적인 학습 가능\n","  * **Soft Actor-Critic SAC** (Week 10)\n","    * **엔트로피** 개념을 도입하여 탐험을 장려하고, **연속적인 행동 공간**에서도 효과적인 학습 가능"],"metadata":{"id":"jfCC82ryUc8k"}},{"cell_type":"markdown","source":["## [실습: Advantage Actor-Critic(A2C)을 이용한 CartPole 실습]"],"metadata":{"id":"TeAsOf0KmVnP"}},{"cell_type":"markdown","source":["### 1. 라이브러리 가져오기 및 하이퍼 파라미터 설정\n","\n","* 필요한 라이브러리 (PyTorch, Gym, NumPy 등)를 가져옵니다.\n","* A2C 알고리즘에 사용될 하이퍼파라미터들을 설정합니다.\n","  * `gamma`: 할인율 (미래 보상의 중요도)\n","  * `learning_rate`: 학습률\n","  * `hidden_size`: 은닉층의 크기 (2층/3층 신경망 선택 가능)\n","  * `num_steps`: 각 롤아웃(rollout)에서 수행할 단계 수\n","    * 롤아웃: Advantage 계산 단위 구간\n","  * `num_episodes`: 총 학습 에피소드 수\n","  * `check_interval`: 학습 진행 상황 확인 주기 (에피소드 단위)\n","  * `early_stop_threshold`: 조기 종료 조건 (평균 보상 기준)\n","  * `early_stop_patience` : 조기 종료 조건 유지 횟수\n","  * `render_gif_path`: 렌더링 결과를 저장할 GIF 파일 경로"],"metadata":{"id":"dK9RQh2ml99C"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.distributions import Categorical\n","import gymnasium as gym\n","import numpy as np\n","import imageio\n","\n","# 하이퍼파라미터\n","gamma = 0.99\n","learning_rate = 0.003\n","hidden_size = 128  # 기본값 (2층 신경망)\n","num_steps = 10\n","num_episodes = 2000\n","progress_interval = 50\n","early_stop_threshold = 400 # CartPole-v1 대체로 200 이상이면 성공으로 분류\n","early_stop_patience = 10\n","render_gif_path = \"cartpole_simulation_a2c.gif\""],"outputs":[],"execution_count":23,"metadata":{"id":"Gx4hBZUQl99E","executionInfo":{"status":"ok","timestamp":1741092720750,"user_tz":-540,"elapsed":3,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 2. A2C 신경망 모델 (Actor-Critic) 정의\n","\n","*   `__init__` 메서드에서 네트워크 구조를 정의\n","  *   `use_three_layers` 인자를 통해 2층 또는 3층 신경망을 선택할 수 있음\n","  *   `fc1`, `fc2` (선택적 `fc_h`), `fc_pi`, `fc_v` 레이어를 정의\n","  *   `fc_pi`는 정책(policy)을 나타내는 확률 분포를 출력\n","  *   `fc_v`는 상태 가치(value)를 출력\n","*   `forward` 메서드에서 입력을 받아 정책과 상태 가치를 반환"],"metadata":{"id":"45Xg0dIPl99F"}},{"cell_type":"code","source":["class ActorCritic(nn.Module):\n","    def __init__(self, input_size, num_actions, hidden_size=128, use_three_layers=False):\n","        super(ActorCritic, self).__init__()\n","        self.use_three_layers = use_three_layers\n","        self.fc1 = nn.Linear(input_size, hidden_size)\n","        if self.use_three_layers:\n","            self.fc_h = nn.Linear(hidden_size, hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, hidden_size)\n","        self.fc_pi = nn.Linear(hidden_size, num_actions)\n","        self.fc_v = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        if self.use_three_layers:\n","            x = F.relu(self.fc_h(x))\n","        x = F.relu(self.fc2(x))\n","        policy = F.softmax(self.fc_pi(x), dim=0)\n","        value = self.fc_v(x)\n","        return policy, value"],"outputs":[],"execution_count":2,"metadata":{"id":"a5jUSMoml99F","executionInfo":{"status":"ok","timestamp":1741091593916,"user_tz":-540,"elapsed":8,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 3.학습 루프 함수 정의\n","\n","**정책/가치 예측 및 행동 선택**\n","* `state_tensor = torch.from_numpy(state).float()`\n","  * 현재 상태(NumPy 배열)를 PyTorch 텐서로 변환\n","* `policy, value = model(state_tensor)`\n","  * 모델에 현재 상태를 입력해 정책과 상태 가치 얻기\n","* `action_dist = Categorical(policy)`\n","  * 정책(확률 분포)을 이용하여 행동을 샘플링하기 위한 `Categorical` 객체를 생성\n","* `action = action_dist.sample()`\n","  * `Categorical` 객체로부터 확률적으로 행동을 선택\n","\n","**선택한 행동을 적용해 환경과 상호작용**\n","* `next_state, reward, done, _ = env.step(action.item())`\n","  * 선택한 행동을 환경에 적용하여 다음 상태, 보상, 종료 여부, 추가 정보 얻기\n","  * `action.item()`은 텐서 형태의 행동을 정수 값으로 변환\n","* `total_reward += reward`: 에피소드 동안의 누적 보상을 계산\n","\n","**경험 저장**\n","* `memory.append((state_tensor, action, reward, value, done))`\n","  * 현재 상태, 선택한 행동, 받은 보상, 상태 가치, 종료 여부를 `memory` 리스트에 튜플 형태로 저장\n","  * 이 데이터는 나중에 Advantage와 손실을 계산하는 데 사용\n","\n","**롤아웃 데이터로부터 Advantage 및 손실 계산**\n","* `loss = 0`: 손실을 초기화\n","* `_, last_value = model(torch.from_numpy(next_state).float())`\n","  * 롤아웃의 마지막 상태(다음 상태)에 대한 가치 함수 값을 계산\n","  * 이 값은 Returns를 계산하는 데 사용\n","* `returns = last_value.detach()`\n","  * 마지막 상태의 가치 함수 값을 Returns의 초기값으로 설정\n","  * `.detach()`는 이 값이 기울기 계산에 포함되지 않도록 함\n","* `for state_tensor, action, reward, value, done in reversed(memory)`\n","  * `memory` 리스트에 저장된 경험들을 역순으로 순회합니다\n","    * 롤아웃의 끝에서부터 시작하여 처음으로 거슬러 올라감\n","  * `returns = reward + gamma * returns * (1 - done)`\n","    * Returns는 현재 보상과 할인된 미래 보상의 합\n","    * 에피소드가 끝났으면(`done=1`), 현재 보상만 Returns가 됨\n","  * `advantage = returns - value`\n","    * Advantage는 Returns와 현재 상태의 가치 함수 값의 차이\n","    * Advantage는 특정 행동이 평균적인 행동보다 얼마나 더 좋거나 나쁜지를 나타냄\n","  * `loss += ~`: 손실 함수를 계산\n","    * `-Categorical(model(state_tensor)[0]).log_prob(action) * advantage.detach()`\n","      * Actor (정책)의 손실\n","      * 정책 그레이디언트(Policy Gradient) 방법을 사용\n","      * Advantage가 높을수록 해당 행동을 선택할 확률을 높이고, Advantage가 낮을수록 확률을 낮춤\n","      * `.detach()`는 Advantage가 기울기 계산에 포함되지 않도록 함\n","    * `F.smooth_l1_loss(value, returns.detach())`\n","      * Critic (가치 함수)의 손실\n","      * Huber Loss를 사용하여 가치 함수 예측값(`value`)과 실제 Returns 간의 차이를 줄임\n","      * `.detach()`는 Returns가 기울기 계산에 포함되지 않도록 함\n","\n","**최적화:**\n","* `optimizer.zero_grad()`\n","  * 옵티마이저에 저장된 이전 기울기 값을 0으로 초기화\n","* `loss.backward()`\n","  * 손실 함수에 대한 역전파(Backpropagation)를 수행하여 모델의 각 파라미터에 대한 기울기를 계산\n","* `optimizer.step()`\n","  * 계산된 기울기를 사용하여 모델의 파라미터를 업데이트 (경사 하강법 적용)\n","* `memory = []`\n","  * 다음 롤아웃을 위해 메모리 비우기"],"metadata":{"id":"PS_MOJy5l99F"}},{"cell_type":"code","source":[],"metadata":{"id":"vuLj1DujKWMR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, optimizer, env, num_episodes, num_steps, gamma, check_interval, early_stop_threshold, early_stop_patience):\n","    scores = []\n","    for episode in range(num_episodes):\n","        state, _ = env.reset()\n","        done = False\n","        total_reward = 0\n","        memory = []\n","\n","        while not done:\n","            for t in range(num_steps):\n","                state_tensor = torch.from_numpy(state).float()\n","                policy, value = model(state_tensor)\n","                action_dist = Categorical(policy)\n","                action = action_dist.sample()\n","\n","                next_state, reward, terminated, truncated, _ = env.step(action.item())\n","                total_reward += reward\n","                done = terminated or truncated\n","\n","                memory.append((state_tensor, action, reward, value, done))\n","                state = next_state\n","\n","                if done:\n","                    break\n","\n","            # 롤아웃 데이터로부터 Advantage 계산 및 손실 함수 계산\n","            loss = 0\n","            _, last_value = model(torch.from_numpy(next_state).float())\n","            returns = last_value.detach()\n","\n","            for state_tensor, action, reward, value, done in reversed(memory):\n","                returns = reward + gamma * returns * (1 - done)\n","                advantage = returns - value\n","                loss += -Categorical(model(state_tensor)[0]).log_prob(action) * advantage.detach() + \\\n","                        F.smooth_l1_loss(value, returns.detach()) # Huber loss\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            memory = [] # 메모리 초기화\n","\n","        scores.append(total_reward)\n","\n","        # 진행 상황 확인 및 조기 종료 검사\n","        if (episode+1) % check_interval == 0:\n","            avg_score = np.mean(scores[-check_interval:])\n","            print(f\"Episode: {episode+1}, Average Reward: {avg_score:.2f}\")\n","\n","        if np.min(scores[-early_stop_patience:]) >= early_stop_threshold:\n","            print(f\"Early stopping triggered! at Episode: {episode+1}\")\n","            break"],"outputs":[],"execution_count":19,"metadata":{"id":"thNHsEHYl99G","executionInfo":{"status":"ok","timestamp":1741092643597,"user_tz":-540,"elapsed":388,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 4. 학습 실행 및 결과 확인\n","\n","* Gym 환경을 생성합니다. (`CartPole-v1`)\n","* `ActorCritic` 모델 인스턴스를 생성 (2층 또는 3층 선택)\n","* `optim.Adam` 옵티마이저를 생성\n","* `train` 함수를 호출하여 학습을 시작"],"metadata":{"id":"rFLnS69kl99G"}},{"cell_type":"code","source":["# 환경 생성\n","env = gym.make('CartPole-v1', render_mode = 'rgb_array')\n","input_size = env.observation_space.shape[0]\n","num_actions = env.action_space.n\n","\n","# 모델 생성 (3층 신경망 사용 예시)\n","model = ActorCritic(input_size, num_actions, hidden_size, use_three_layers=True)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# 학습 실행\n","train(model, optimizer, env, num_episodes, num_steps, gamma, progress_interval, early_stop_threshold, early_stop_patience)\n","\n","env.close()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 50, Average Reward: 16.78\n","Episode: 100, Average Reward: 24.02\n","Episode: 150, Average Reward: 18.66\n","Episode: 200, Average Reward: 37.02\n","Episode: 250, Average Reward: 39.84\n","Episode: 300, Average Reward: 31.26\n","Episode: 350, Average Reward: 55.00\n","Episode: 400, Average Reward: 104.56\n","Early stopping triggered! at Episode: 422\n"]}],"execution_count":24,"metadata":{"id":"__Cb13DFl99G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741092778925,"user_tz":-540,"elapsed":54279,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}},"outputId":"d35dae69-4e98-4092-c31f-0d5601e2a3e7"}},{"cell_type":"markdown","source":["### 5. 학습된 모델 테스트 및 GIF 렌더링\n","\n","* 학습이 완료된 모델을 사용하여 한 번의 에피소드를 실행\n","* 각 스텝의 화면을 수집\n","* `imageio` 라이브러리를 사용하여 GIF를 생성\n","* 에피소드 동안 얻은 총 보상을 출력"],"metadata":{"id":"yO8gOBQIl99G"}},{"cell_type":"code","source":["state, _ = env.reset()\n","done = False\n","total_reward = 0\n","frames = []\n","\n","with torch.no_grad():  # 기울기 계산 비활성화 (추론 모드)\n","    while not done:\n","        state_tensor = torch.from_numpy(state).float()\n","        policy, _ = model(state_tensor)\n","        action_dist = Categorical(policy)\n","        action = action_dist.sample()\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action.item())\n","        total_reward += reward\n","        state = next_state\n","        done = terminated or truncated\n","\n","        # 렌더링 이미지를 프레임 리스트에 추가\n","        frames.append(env.render())\n","\n","# GIF 파일로 저장\n","imageio.mimsave(render_gif_path, frames, duration=33)  # fps 조절 가능 duration = 1000/fps\n","print(f\"Test Episode Reward: {total_reward}\")\n","env.close()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Test Episode Reward: 500.0\n"]}],"execution_count":25,"metadata":{"id":"w3NsvCPdl99H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741092810158,"user_tz":-540,"elapsed":25200,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}},"outputId":"b18d1b68-526e-4e0f-fcf8-4ae6aaf3f8b9"}}]}