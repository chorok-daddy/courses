{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNYY58ksrOGzrRujhh/otal"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 2주차 가치 기반 접근 방식"],"metadata":{"id":"38soBLZhJ4ab"}},{"cell_type":"markdown","source":["## [핵심 목표]\n","* **Q 함수(Q-value)**를 통해 상태-행동 쌍의 가치를 정의하고, 벨만 방정식을 사용하여 Q 함수를 업데이트하는 개념을 이해\n","\n","* PyTorch로 Q 함수를 근사하는 신경망 모델을 구현 준비\n","\n","## [가치 기반 접근 (Value-based Approach)]\n","정책을 직접 파라미터화하지 않고, 가치 함수 또는 Q 함수를 학습\n","\n","### 1. $Q$ 함수 - $Q(s, a)$\n","* 상태 s에서 행동 a를 취했을 때 앞으로 얻을 수 있는 예상 누적 보상 (Expected Return)\n","\n","* \"상태 s에서 행동 a를 하면 얼마나 좋은가?\"를 정량화\n","Q 함수를 정확히 알면 각 상태에서 가치가 가장 높은 행동을 선택하는 정책을 쉽게 생성 가능\n","\n","### 2. $Q$-러닝 ($Q$-learning)\n","* **벨만 최적 방정식 (Bellman Optimality Equation)**을 사용하여 Q 함수를 업데이트\n","\n","* $Q^*(s,a) = \\mathbb{E}[ r + \\gamma \\max_{a'} Q^*(s', a') ]$\n","\n","  * $s'$: 다음 상태\n","  * $r$: 보상\n","  * $\\gamma$: 할인율 (0 ~ 1)\n","  * $\\max_{a'}$: 다음 상태에서 가장 높은 Q값을 주는 행동 선택\n","* $Q$-러닝: 벨만 최적 방정식을 사용하여 Q 함수를 갱신하는 오프폴리시(Off-policy) 알고리즘\n","  * 업데이트 규칙\n","    $$Q(s,a) \\leftarrow Q(s,a) + \\alpha [ r + \\gamma \\max_{a'}Q(s',a') - Q(s,a) ]$$\n","    * $\\alpha$: 학습률 (Learning rate)\n","    * $(s, a, r, s')$: 경험 샘플\n","\n","### 3. PyTorch 신경망 모델 구현 준비\n","* Q 함수를 딥뉴럴넷으로 근사하여 복잡한 환경에서도 확장성을 확보\n","* 다음 단계로 경험 리플레이(Replay Buffer), 타겟 네트워크(Target Network) 등의 개념을 도입하여 DQN 알고리즘을 구현하고, Q 함수를 실제로 학습시키는 과정 진행 예정"],"metadata":{"id":"z9Dkb-QoRwkw"}},{"cell_type":"markdown","source":["## [Q-러닝 실습 예제: Q-러닝으로 미로 탈출하기]"],"metadata":{"id":"6gnt1KKwKZ5u"}},{"cell_type":"markdown","source":["### 1. 환경 설정"],"metadata":{"id":"NKZSHJ4jKbPc"}},{"cell_type":"markdown","source":["#### 미로 정의\n","\n","5x5 크기의 미로를 numpy 배열로 표현\n","\n","* 0: 길\n","* 1: 벽\n","* 2: 시작\n","* 3: 목표"],"metadata":{"id":"fXtiW_7Ve9jP"}},{"cell_type":"code","source":["import numpy as np\n","\n","# 5x5 미로 환경 설정 (0: 길, 1: 벽, 2: 시작, 3: 목표)\n","maze = np.array([\n","    [2, 0, 0, 0, 0],\n","    [0, 1, 1, 0, 0],\n","    [0, 0, 0, 1, 0],\n","    [1, 1, 0, 1, 0],\n","    [0, 1, 0, 3, 0]\n","])"],"outputs":[],"execution_count":43,"metadata":{"id":"tkq3c0Ape9jQ","executionInfo":{"status":"ok","timestamp":1740657442537,"user_tz":-540,"elapsed":1,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["#### 행동 정의\n","\n","에이전트가 취할 수 있는 행동을 정의\n","\n","* 0: 위\n","* 1: 아래\n","* 2: 왼쪽\n","* 3: 오른쪽"],"metadata":{"id":"Hc9RIvwye9jQ"}},{"cell_type":"code","source":["# 0: 위로 1칸 이동(-1,0)\n","# 1: 아래로 1칸 이동(1,0)\n","# 2: 왼쪽으로 1칸 이동(0,-1)\n","# 3: 오른쪽으로 1칸 이동(0,1))\n","actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]"],"outputs":[],"execution_count":33,"metadata":{"id":"LjpZKPtze9jQ","executionInfo":{"status":"ok","timestamp":1740657223786,"user_tz":-540,"elapsed":9,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["#### Q테이블 초기화\n","\n","Q테이블을 0으로 초기화 (5x5x4개행동)"],"metadata":{"id":"VgoupMnhe9jQ"}},{"cell_type":"code","source":["q_table = np.zeros((5, 5, 4))"],"outputs":[],"execution_count":49,"metadata":{"id":"WrHCA7E4e9jR","executionInfo":{"status":"ok","timestamp":1740657484054,"user_tz":-540,"elapsed":3,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["#### 하이퍼파라미터 설정\n","\n","학습에 필요한 하이퍼파라미터를 설정\n","\n","* `learning_rate`: 학습률\n","* `discount_factor`: 할인율\n","* `epsilon`: 탐험 비율\n","* `episodes`: 에피소드 수"],"metadata":{"id":"jR0UQTUne9jR"}},{"cell_type":"code","source":["learning_rate = 0.1\n","discount_factor = 0.9\n","epsilon = 0.1  # 탐험 비율\n","episodes = 1000"],"outputs":[],"execution_count":45,"metadata":{"id":"frukJ_Tee9jR","executionInfo":{"status":"ok","timestamp":1740657446914,"user_tz":-540,"elapsed":1,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 2. Q-러닝 알고리즘"],"metadata":{"id":"XCSHDkLUe9jR"}},{"cell_type":"markdown","source":["#### 다음 상태 얻기\n","\n","현재 상태와 행동을 입력받아 다음 상태를 반환하는 함수"],"metadata":{"id":"yVY2U_4vKUWp"}},{"cell_type":"code","source":["def get_next_state(state, action):\n","  next_state = (state[0] + actions[action][0], state[1] + actions[action][1])\n","\n","  # 경계 확인 및 벽 충돌 감지\n","  if 0 <= next_state[0] < 5 and 0 <= next_state[1] < 5 and maze[next_state[0], next_state[1]] != 1:\n","    return next_state\n","  else:\n","    return state"],"outputs":[],"execution_count":50,"metadata":{"id":"82tPue_2e9jR","executionInfo":{"status":"ok","timestamp":1740657486311,"user_tz":-540,"elapsed":1,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["#### 보상 얻기\n","\n","현재 상태를 입력받아 보상을 반환하는 함수\n","\n","* 목표 상태: 10\n","* 그 외: -1"],"metadata":{"id":"EXCYvcTle9jR"}},{"cell_type":"code","source":["def get_reward(state):\n","  if maze[state[0], state[1]] == 3:\n","    return 10\n","  else:\n","    return -1"],"outputs":[],"execution_count":51,"metadata":{"id":"Dj34MIHVe9jR","executionInfo":{"status":"ok","timestamp":1740657487538,"user_tz":-540,"elapsed":8,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["#### Q-러닝 학습\n","\n","Q-러닝 알고리즘을 사용하여 Q-테이블을 업데이트"],"metadata":{"id":"3-csKWmme9jR"}},{"cell_type":"code","source":["# Q-러닝 알고리즘\n","for episode in range(episodes):\n","  state = (0, 0)  # 시작 위치\n","  while maze[state[0], state[1]] != 3 :\n","    if np.random.rand() < epsilon:\n","        action = np.random.randint(4)\n","    else:\n","        action = np.argmax(q_table[state[0], state[1], :])\n","\n","    next_state = get_next_state(state, action)\n","    reward = get_reward(next_state)\n","\n","    q_table[state[0], state[1], action] += learning_rate * (\n","            reward + discount_factor * np.max(q_table[next_state[0], next_state[1], :])\n","            - q_table[state[0], state[1], action]\n","    )\n","\n","    state = next_state\n","\n","    # 중간 프로그래스 출력 (예시: 100번째 에피소드마다 출력)\n","  if (episode + 1) % 100 == 0:\n","    print(f\"Episode {episode + 1} 완료\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 100 완료\n","Episode 200 완료\n","Episode 300 완료\n","Episode 400 완료\n","Episode 500 완료\n","Episode 600 완료\n","Episode 700 완료\n","Episode 800 완료\n","Episode 900 완료\n","Episode 1000 완료\n"]}],"execution_count":52,"metadata":{"id":"syt-O5xze9jR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740657489045,"user_tz":-540,"elapsed":72,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}},"outputId":"178b110f-1ea5-4309-be8e-f1cb2588af60"}},{"cell_type":"markdown","source":["##### Q-값 업데이트 수식\n","\n","$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$$\n","\n","* $Q(s, a)$: 상태 $s$에서 행동 $a$의 Q-값\n","* $\\alpha$: 학습률\n","* $r$: 보상\n","* $\\gamma$: 할인율\n","* $\\max_{a'} Q(s', a')$: 다음 상태 $s'$에서 최대 Q-값\n"],"metadata":{"id":"RfsPsHTIe9jR"}},{"cell_type":"markdown","source":["### 3. 결과 확인"],"metadata":{"id":"HMK-2zhuKnKU"}},{"cell_type":"markdown","source":["#### 최종 경로 찾기\n","\n","학습된 Q-테이블을 사용하여 시작 상태에서 목표 상태까지의 최적 경로를 찾기"],"metadata":{"id":"rvN1G2gKf13S"}},{"cell_type":"code","source":["# 최종 경로 찾기\n","state = (0, 0)\n","path = [state]\n","while maze[state[0], state[1]] != 3:\n","    action = np.argmax(q_table[state[0], state[1], :])\n","    state = get_next_state(state, action)\n","    path.append(state)"],"outputs":[],"execution_count":53,"metadata":{"id":"haj8mF17e9jR","executionInfo":{"status":"ok","timestamp":1740657493003,"user_tz":-540,"elapsed":3,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["#### 경로 시각화\n","\n","찾은 경로를 text로 시각화하여 출력"],"metadata":{"id":"muZgHqMJe9jR"}},{"cell_type":"code","source":["# 경로 시각화\n","print(\"미로 탈출 경로:\")\n","for i in range(5):\n","    for j in range(5):\n","        if (i, j) == (0, 0):\n","          print(\"S\", end=\" \")\n","        elif (i,j) == (4,3):\n","          print(\"G\", end=\" \")\n","        elif (i,j) in path:\n","          print(\"*\", end=\" \")  # 지나간 경로\n","        elif maze[i, j] == 1:\n","            print(\"#\", end=\" \")  # 벽 표시\n","        else:\n","            print(\" \", end=\" \")\n","    print()"],"outputs":[{"output_type":"stream","name":"stdout","text":["미로 탈출 경로:\n","S         \n","* # #     \n","* * * #   \n","# # * #   \n","  # * G   \n"]}],"execution_count":57,"metadata":{"id":"C9ZriypRe9jS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740657560955,"user_tz":-540,"elapsed":9,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}},"outputId":"e53c6445-56a3-4de4-cf36-39686b505ef0"}}]}