{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNTqR9oPpnWNtP6EXttZe/G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 4주차 Double DQN"],"metadata":{"id":"PGXSM98Arkyk"}},{"cell_type":"markdown","source":["## [핵심 목표]\n","* Double DQN의 개념을 이해하고, Q값 과대 추정(Overestimation) 문제를 완화하는 방법을 학습\n","* Double DQN을 기존 DQN 코드에 적용하는 방법을 이해"],"metadata":{"id":"5krtZFCnrpR6"}},{"cell_type":"markdown","source":["## [Q값 과대 추정 (Overestimation) 문제]\n","\n","* DQN은 최대 Q값을 직접 사용하기 때문에, 노이즈나 학습 초기 불안정성으로 인해 실제보다 높은 Q값을 선택하는 경향이 있음\n","* 이로 인해 정책이 왜곡될 수 있음"],"metadata":{"id":"013sz_DQrq3k"}},{"cell_type":"markdown","source":["### 1. Q값 과대 추정 문제 예시: 게임 상황\n","\n","* **문제**: 간단한 미로 탈출 게임\n","* **목표**: 미로를 탈출하여 보상을 얻는 것\n","* **에이전트**: DQN 알고리즘을 사용하여 학습하는 캐릭터\n"],"metadata":{"id":"1u5IszV5_JkO"}},{"cell_type":"markdown","source":["### 2. DQN의 과대 추정 문제 발생 과정\n","\n","* 학습 초기 단계에서 에이전트는 우연히 미로의 특정 구역에 도달했을 때 높은 Q값을 얻음\n","* 이 높은 Q값은 실제 보상보다 과대 추정된 값일 수 있음\n","  * 이 구역에 도달한 것이 가치가 있어서 Q값이 높은 것이 아닐 수 있음\n","  * 달리 말하면, 아직 다른 가치 있는 구역을 모르는 것일 뿐일 수 있음\n","* 에이전트는 이 과대 추정된 Q값을 '진짜 가치가 있다'라고 믿고 해당 구역을 최적의 경로라고 판단"],"metadata":{"id":"hsiF1EbUD95o"}},{"cell_type":"markdown","source":["### 3. 정책 왜곡 및 문제점\n","\n","* 실제로 해당 구역은 최적의 경로가 아닐 수 있고, 더 짧거나 안전한 다른 경로가 존재할 수 있음\n","* 하지만 에이전트는 과대 추정된 Q값 때문에 계속해서 해당 구역으로 향하는 잘못된 결정을 내림\n","* 결과적으로 에이전트는 최적의 경로를 학습하지 못하고 미로 탈출에 실패하거나 더 오랜 시간이 걸리게 됨"],"metadata":{"id":"RtGe2eJeD_xH"}},{"cell_type":"markdown","source":["## [Double DQN 아이디어]\n","\n","* 행동 선택과 Q값 평가를 분리\n","* 기존 DQN: 메인 네트워크를 사용하여 행동을 선택하고, 타겟 네트워크를 사용하여 선택된 행동의 Q값을 평가\n","* Double DQN:\n","  * 메인 네트워크를 사용하여 최적 행동 $a^*$ 을 선택\n","  * 타겟 네트워크를 사용하여 선택된 행동 $a^*$ 의 Q값을 평가"],"metadata":{"id":"xpoW6pXCrtlp"}},{"cell_type":"markdown","source":["## [Double DQN 학습 업데이트]\n","\n","* Temporal Difference Tagert (TD Target):\n","  * 기존 DQN: $r + \\gamma \\underset{a'}{max\\;} Q(s', a'; \\theta^-)$\n","  * Double DQN: $r + \\gamma Q(s', \\underset{a}{arg \\text{ } max \\text{ }} Q(s', a; \\theta); \\theta^-)$\n","    * $r$: 즉각적인 보상\n","    * $\\gamma$: 할인율\n","    * $s'$: 다음 상태\n","    * $\\theta$: 메인 네트워크의 파라미터\n","    * $\\theta^-$: 타겟 네트워크의 파라미터\n","    * $\\underset{a}{arg \\text{ } max \\text{ }} Q(s', a; \\theta)$: 메인 네트워크를 사용하여 Q값이 최대가 되는 행동 $a$ 선택\n","    * $Q(s', \\underset{a}{arg \\text{ } max \\text{ }} Q(s', a; \\theta); \\theta^-)$: 선택된 행동 $a$에 대한 타겟 네트워크의 Q값 평가\n","* 손실 함수 (Loss Function): TD 에러를 최소화하는 방향으로 학습\n","  * TD 에러: 현재 네트워크 예측과 실제 타겟 간의 차이\n","  * $Error_{TD, DQN} = Q(s, a; \\theta) - (r + \\gamma \\underset{a'}{max} \\text{ } Q(s', a'; \\theta^-))$\n","    * $\\underset{a'}{max} \\text{ } Q(s', a'; \\theta^-)$: 최대 Q값의 선택과 Q값 평가에 동일한 네트워크($\\theta^-$) 사용\n","    * Q값 과대 추정 문제 발생\n","  * $Error_{TD, DDQN} = Q(s, a; \\theta) - (r + \\gamma Q(s', \\underset{a}{arg \\text{ } max \\text{ }} Q(s', a; \\theta); \\theta^-))$\n","    * $\\underset{a}{arg \\text{ } max \\text{ }} Q(S', a'; \\theta)$: 메인 네트워크($\\theta$)를 사용하여 다음 상태($S'$)에서 가장 높은 Q값을 가지는 행동($a'$)을 선택\n","    * $Q(S', \\underset{a}{arg \\text{ } max \\text{ }} Q(S', a'; \\theta); \\theta')$: 선택된 행동($a'$)의 Q값을 타겟 네트워크($θ'$)를 사용해 평가\n","    * 이 차이가 Q값 과대 추정 문제를 완화하는 데 중요한 역할을 함\n","* 업데이트: 손실 함수를 최소화하는 방향으로 메인 네트워크의 파라미터를 업데이트\n","\n","\n","\n"],"metadata":{"id":"Qz9VzGN9rwTn"}},{"cell_type":"markdown","source":["## [실습: Double DQN을 이용한 CartPole 학습]"],"metadata":{"id":"mR6UTWG9zfX8"}},{"cell_type":"markdown","source":["### 1. 환경 설정\n","\n","* `gym`: CartPole 환경 제공\n","* `torch`: 딥러닝 모델 및 연산 처리\n","* `numpy`: 배열 및 수치 연산\n","* `collections.deque`: 경험 리플레이 버퍼 구현"],"metadata":{"id":"WNEdakvZzqZs"}},{"cell_type":"code","source":["import gym\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from collections import deque\n","\n","env = gym.make('CartPole-v1')\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n"],"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}],"execution_count":null,"metadata":{"id":"DLvIRoaLzfX-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740666552502,"user_tz":-540,"elapsed":12039,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}},"outputId":"9e666d33-7a41-42cd-cb44-59a0aacfbb9e"}},{"cell_type":"markdown","source":["### 2. DQN 모델 정의\n","\n","* `DQNAgent` 클래스: 신경망 모델 정의\n","* 3개의 완전 연결 계층 (`nn.Linear`) 사용\n","* ReLU 활성화 함수 적용 (`torch.relu`)\n","* 입력: 상태, 출력: 각 행동의 Q값"],"metadata":{"id":"NcsVjvstzfX_"}},{"cell_type":"code","source":["class DQNAgent(nn.Module):\n","    def __init__(self, state_size, action_size):\n","        super(DQNAgent, self).__init__()\n","        self.fc1 = nn.Linear(state_size, 24)\n","        self.fc2 = nn.Linear(24, 24)\n","        self.fc3 = nn.Linear(24, action_size)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        return self.fc3(x)"],"outputs":[],"execution_count":null,"metadata":{"id":"Ff1r1-_pzfX_"}},{"cell_type":"markdown","source":["### 3. 하이퍼파라미터 설정\n","\n","* 학습률 (`learning_rate`), 할인율 (`gamma`), 탐험 확률 (`epsilon`) 등 설정\n","* 미니배치 크기 (`batch_size`), 메모리 크기 (`memory_size`) 설정\n","* 타겟 네트워크 업데이트 주기 (`target_update_frequency`) 설정"],"metadata":{"id":"hm481-1_zfYA"}},{"cell_type":"code","source":["learning_rate = 0.001\n","gamma = 0.99  # 할인율\n","epsilon = 1.0  # 초기 탐험 확률\n","epsilon_min = 0.01\n","epsilon_decay = 0.999\n","batch_size = 64\n","memory_size = 10000\n","target_update_frequency = 10  # 타겟 네트워크 업데이트 주기"],"outputs":[],"execution_count":null,"metadata":{"id":"PpKXIXIvzfYA"}},{"cell_type":"markdown","source":["### 4. DQN 에이전트 및 타겟 네트워크 생성\n","\n","* `DQNAgent` 모델 인스턴스 생성 (agent, target_agent)\n","* 타겟 네트워크 초기화 (agent 네트워크의 가중치 복사)\n","* Adam optimizer 및 MSE loss function 설정"],"metadata":{"id":"KmN6mOPtzfYA"}},{"cell_type":"code","source":["agent = DQNAgent(state_size, action_size)\n","target_agent = DQNAgent(state_size, action_size)\n","target_agent.load_state_dict(agent.state_dict())\n","optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n","criterion = nn.MSELoss()"],"outputs":[],"execution_count":null,"metadata":{"id":"9WoPFSe7zfYA"}},{"cell_type":"markdown","source":["### 5. 경험 리플레이 버퍼 초기화\n","\n","* `deque`를 사용하여 경험 리플레이 버퍼 (`memory`) 초기화"],"metadata":{"id":"0SpkNpCRzfYA"}},{"cell_type":"code","source":["memory = deque(maxlen=memory_size)"],"outputs":[],"execution_count":null,"metadata":{"id":"1JiN4UTEzfYA"}},{"cell_type":"markdown","source":["### 6. 학습 함수 (`train`)\n","\n","* 경험 리플레이 버퍼에서 미니배치 샘플링\n","* 상태, 행동, 보상, 다음 상태, 종료 여부 추출\n","* **Double DQN 핵심**: agent 네트워크에서 다음 상태의 최대 Q-값을 가지는 행동 선택, target 네트워크에서 선택된 행동의 Q-값 계산\n","* 예측 Q-값과 타겟 Q-값 비교하여 손실 함수 계산\n","* 역전파 및 가중치 업데이트"],"metadata":{"id":"-4UPvXr3zfYB"}},{"cell_type":"code","source":["def train(batch_size):\n","    minibatch = random.sample(memory, batch_size)\n","    states = torch.tensor(np.array([transition[0] for transition in minibatch]), dtype=torch.float32)\n","    actions = torch.tensor(np.array([transition[1] for transition in minibatch]), dtype=torch.long)\n","    rewards = torch.tensor(np.array([transition[2] for transition in minibatch]), dtype=torch.float32)\n","    next_states = torch.tensor(np.array([transition[3] for transition in minibatch]), dtype=torch.float32)\n","    dones = torch.tensor(np.array([transition[4] for transition in minibatch]), dtype=torch.float32)\n","\n","    # Double DQN 핵심: agent 네트워크에서 다음 상태의 최대 Q-값을 가지는 행동 선택\n","    next_actions = agent(next_states).argmax(1).unsqueeze(1)\n","    # target 네트워크에서 선택된 행동의 Q-값 계산\n","    target_q_values = target_agent(next_states).gather(1, next_actions).squeeze()\n","    targets = rewards + gamma * target_q_values * (1 - dones)\n","\n","    predicted_q_values = agent(states).gather(1, actions.unsqueeze(1)).squeeze()\n","    loss = criterion(predicted_q_values, targets)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()"],"outputs":[],"execution_count":null,"metadata":{"id":"ekMJ9PThzfYB"}},{"cell_type":"markdown","source":["### 7. 학습 루프\n","\n","* `episodes`만큼 에피소드 반복\n","* 각 에피소드마다 환경 초기화 및 상태 설정\n","* ε-탐욕적 정책으로 행동 선택\n","* 환경에서 행동 실행 후 다음 상태, 보상, 종료 여부 얻음\n","* 경험 리플레이 버퍼에 경험 추가\n","* 버퍼 크기가 `batch_size` 이상이면 학습 함수 호출\n","* 타겟 네트워크 업데이트 및 탐험 확률 감소\n","* 에피소드 결과 출력"],"metadata":{"id":"6aSovNuszfYB"}},{"cell_type":"code","source":["# 학습 루프\n","episodes = 100\n","for e in range(episodes):\n","    state = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32)\n","    done = False\n","    total_reward = 0\n","\n","    while not done:\n","        if random.random() < epsilon:\n","            action = env.action_space.sample()\n","        else:\n","            action = agent(state).argmax().item()\n","        next_state, reward, done, _ = env.step(action)\n","        next_state = torch.tensor(next_state, dtype=torch.float32)\n","\n","        total_reward += reward\n","        memory.append((state, action, reward, next_state, done))\n","\n","        if len(memory) >= batch_size:\n","            train(batch_size)\n","\n","        state = next_state\n","        if e % target_update_frequency == 0:\n","            target_agent.load_state_dict(agent.state_dict())\n","\n","        if epsilon > epsilon_min:\n","            epsilon *= epsilon_decay\n","\n","    print(f\"Episode {e + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1/100, Total Reward: 21.0, Epsilon: 0.979\n","Episode 2/100, Total Reward: 30.0, Epsilon: 0.950\n","Episode 3/100, Total Reward: 27.0, Epsilon: 0.925\n","Episode 4/100, Total Reward: 16.0, Epsilon: 0.910\n","Episode 5/100, Total Reward: 29.0, Epsilon: 0.884\n","Episode 6/100, Total Reward: 8.0, Epsilon: 0.877\n","Episode 7/100, Total Reward: 20.0, Epsilon: 0.860\n","Episode 8/100, Total Reward: 13.0, Epsilon: 0.849\n","Episode 9/100, Total Reward: 18.0, Epsilon: 0.834\n","Episode 10/100, Total Reward: 20.0, Epsilon: 0.817\n","Episode 11/100, Total Reward: 18.0, Epsilon: 0.802\n","Episode 12/100, Total Reward: 40.0, Epsilon: 0.771\n","Episode 13/100, Total Reward: 19.0, Epsilon: 0.756\n","Episode 14/100, Total Reward: 42.0, Epsilon: 0.725\n","Episode 15/100, Total Reward: 49.0, Epsilon: 0.691\n","Episode 16/100, Total Reward: 49.0, Epsilon: 0.658\n","Episode 17/100, Total Reward: 92.0, Epsilon: 0.600\n","Episode 18/100, Total Reward: 22.0, Epsilon: 0.587\n","Episode 19/100, Total Reward: 40.0, Epsilon: 0.564\n","Episode 20/100, Total Reward: 38.0, Epsilon: 0.543\n","Episode 21/100, Total Reward: 113.0, Epsilon: 0.485\n","Episode 22/100, Total Reward: 211.0, Epsilon: 0.392\n","Episode 23/100, Total Reward: 19.0, Epsilon: 0.385\n","Episode 24/100, Total Reward: 21.0, Epsilon: 0.377\n","Episode 25/100, Total Reward: 500.0, Epsilon: 0.229\n","Episode 26/100, Total Reward: 500.0, Epsilon: 0.139\n","Episode 27/100, Total Reward: 239.0, Epsilon: 0.109\n","Episode 28/100, Total Reward: 245.0, Epsilon: 0.085\n","Episode 29/100, Total Reward: 69.0, Epsilon: 0.080\n","Episode 30/100, Total Reward: 226.0, Epsilon: 0.064\n","Episode 31/100, Total Reward: 147.0, Epsilon: 0.055\n","Episode 32/100, Total Reward: 170.0, Epsilon: 0.046\n","Episode 33/100, Total Reward: 147.0, Epsilon: 0.040\n","Episode 34/100, Total Reward: 181.0, Epsilon: 0.033\n","Episode 35/100, Total Reward: 157.0, Epsilon: 0.029\n","Episode 36/100, Total Reward: 145.0, Epsilon: 0.025\n","Episode 37/100, Total Reward: 149.0, Epsilon: 0.021\n","Episode 38/100, Total Reward: 147.0, Epsilon: 0.018\n","Episode 39/100, Total Reward: 141.0, Epsilon: 0.016\n","Episode 40/100, Total Reward: 151.0, Epsilon: 0.014\n","Episode 41/100, Total Reward: 130.0, Epsilon: 0.012\n","Episode 42/100, Total Reward: 127.0, Epsilon: 0.011\n","Episode 43/100, Total Reward: 132.0, Epsilon: 0.010\n","Episode 44/100, Total Reward: 141.0, Epsilon: 0.010\n","Episode 45/100, Total Reward: 127.0, Epsilon: 0.010\n","Episode 46/100, Total Reward: 142.0, Epsilon: 0.010\n","Episode 47/100, Total Reward: 126.0, Epsilon: 0.010\n","Episode 48/100, Total Reward: 127.0, Epsilon: 0.010\n","Episode 49/100, Total Reward: 119.0, Epsilon: 0.010\n","Episode 50/100, Total Reward: 117.0, Epsilon: 0.010\n","Episode 51/100, Total Reward: 110.0, Epsilon: 0.010\n","Episode 52/100, Total Reward: 107.0, Epsilon: 0.010\n","Episode 53/100, Total Reward: 115.0, Epsilon: 0.010\n","Episode 54/100, Total Reward: 110.0, Epsilon: 0.010\n","Episode 55/100, Total Reward: 110.0, Epsilon: 0.010\n","Episode 56/100, Total Reward: 110.0, Epsilon: 0.010\n","Episode 57/100, Total Reward: 114.0, Epsilon: 0.010\n","Episode 58/100, Total Reward: 113.0, Epsilon: 0.010\n","Episode 59/100, Total Reward: 113.0, Epsilon: 0.010\n","Episode 60/100, Total Reward: 110.0, Epsilon: 0.010\n","Episode 61/100, Total Reward: 125.0, Epsilon: 0.010\n","Episode 62/100, Total Reward: 139.0, Epsilon: 0.010\n","Episode 63/100, Total Reward: 162.0, Epsilon: 0.010\n","Episode 64/100, Total Reward: 176.0, Epsilon: 0.010\n","Episode 65/100, Total Reward: 143.0, Epsilon: 0.010\n","Episode 66/100, Total Reward: 133.0, Epsilon: 0.010\n","Episode 67/100, Total Reward: 136.0, Epsilon: 0.010\n","Episode 68/100, Total Reward: 134.0, Epsilon: 0.010\n","Episode 69/100, Total Reward: 70.0, Epsilon: 0.010\n","Episode 70/100, Total Reward: 129.0, Epsilon: 0.010\n","Episode 71/100, Total Reward: 143.0, Epsilon: 0.010\n","Episode 72/100, Total Reward: 11.0, Epsilon: 0.010\n","Episode 73/100, Total Reward: 15.0, Epsilon: 0.010\n","Episode 74/100, Total Reward: 13.0, Epsilon: 0.010\n","Episode 75/100, Total Reward: 98.0, Epsilon: 0.010\n","Episode 76/100, Total Reward: 192.0, Epsilon: 0.010\n","Episode 77/100, Total Reward: 79.0, Epsilon: 0.010\n","Episode 78/100, Total Reward: 76.0, Epsilon: 0.010\n","Episode 79/100, Total Reward: 77.0, Epsilon: 0.010\n","Episode 80/100, Total Reward: 82.0, Epsilon: 0.010\n","Episode 81/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 82/100, Total Reward: 132.0, Epsilon: 0.010\n","Episode 83/100, Total Reward: 141.0, Epsilon: 0.010\n","Episode 84/100, Total Reward: 139.0, Epsilon: 0.010\n","Episode 85/100, Total Reward: 145.0, Epsilon: 0.010\n","Episode 86/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 87/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 88/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 89/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 90/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 91/100, Total Reward: 162.0, Epsilon: 0.010\n","Episode 92/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 93/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 94/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 95/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 96/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 97/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 98/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 99/100, Total Reward: 500.0, Epsilon: 0.010\n","Episode 100/100, Total Reward: 500.0, Epsilon: 0.010\n"]}],"execution_count":null,"metadata":{"id":"Xz1zE8UfzfYB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740669687565,"user_tz":-540,"elapsed":61636,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}},"outputId":"7cbbe43c-f60d-422d-dd52-e6f70147ace2"}},{"cell_type":"markdown","source":["### 8. 학습된 에이전트 검증 (렌더링 with GIF)"],"metadata":{"id":"AkHIhwtpV3Ng"}},{"cell_type":"code","source":["import imageio\n","import os\n","\n","frames = []\n","state = env.reset()\n","state = torch.tensor(state, dtype=torch.float32)\n","done = False\n","for i in range(3000): # 3000 steps simulation\n","  action = agent(state).argmax().item()\n","  next_state, reward, done, _ = env.step(action)\n","  next_state = torch.tensor(next_state, dtype=torch.float32)\n","  state = next_state\n","\n","  if i % 10 == 0: # Render every 10 steps\n","    frames.append(env.render(mode='rgb_array'))\n","\n","  if done:\n","    break\n","\n","imageio.mimsave('cartpole_simulation_doubleDQN.gif', frames, duration=33) # save to gif file\n","print(\"GIF saved as cartpole_simulation.gif\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fQdsiRmOevyM","executionInfo":{"status":"ok","timestamp":1740669718152,"user_tz":-540,"elapsed":2485,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}},"outputId":"96f0c133-3419-47a6-bd62-c989d29c944d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GIF saved as cartpole_simulation.gif\n"]}]}]}