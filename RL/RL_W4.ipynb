{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNMzCPIy+PonbDwYhLF9Sf+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 4주차 Double DQN"],"metadata":{"id":"PGXSM98Arkyk"}},{"cell_type":"markdown","source":["## [핵심 목표]\n","* Double DQN의 개념을 이해하고, Q값 과대 추정(Overestimation) 문제를 완화하는 방법을 학습\n","* Double DQN을 기존 DQN 코드에 적용하는 방법을 이해"],"metadata":{"id":"5krtZFCnrpR6"}},{"cell_type":"markdown","source":["## [Q값 과대 추정 (Overestimation) 문제]\n","\n","* DQN은 최대 Q값을 직접 사용하기 때문에, 노이즈나 학습 초기 불안정성으로 인해 실제보다 높은 Q값을 선택하는 경향이 있음\n","* 이로 인해 정책이 왜곡될 수 있음"],"metadata":{"id":"013sz_DQrq3k"}},{"cell_type":"markdown","source":["### 1. Q값 과대 추정 문제 예시: 게임 상황\n","\n","* **문제**: 간단한 미로 탈출 게임\n","* **목표**: 미로를 탈출하여 보상을 얻는 것\n","* **에이전트**: DQN 알고리즘을 사용하여 학습하는 캐릭터\n"],"metadata":{"id":"1u5IszV5_JkO"}},{"cell_type":"markdown","source":["### 2. DQN의 과대 추정 문제 발생 과정\n","\n","* 학습 초기 단계에서 에이전트는 우연히 미로의 특정 구역에 도달했을 때 높은 Q값을 얻음\n","* 이 높은 Q값은 실제 보상보다 과대 추정된 값일 수 있음\n","  * 이 구역에 도달한 것이 가치가 있어서 Q값이 높은 것이 아닐 수 있음\n","  * 달리 말하면, 아직 다른 가치 있는 구역을 모르는 것일 뿐일 수 있음\n","* 에이전트는 이 과대 추정된 Q값을 '진짜 가치가 있다'라고 믿고 해당 구역을 최적의 경로라고 판단"],"metadata":{"id":"hsiF1EbUD95o"}},{"cell_type":"markdown","source":["### 3. 정책 왜곡 및 문제점\n","\n","* 실제로 해당 구역은 최적의 경로가 아닐 수 있고, 더 짧거나 안전한 다른 경로가 존재할 수 있음\n","* 하지만 에이전트는 과대 추정된 Q값 때문에 계속해서 해당 구역으로 향하는 잘못된 결정을 내림\n","* 결과적으로 에이전트는 최적의 경로를 학습하지 못하고 미로 탈출에 실패하거나 더 오랜 시간이 걸리게 됨"],"metadata":{"id":"RtGe2eJeD_xH"}},{"cell_type":"markdown","source":["## [Double DQN 아이디어]\n","\n","* 행동 선택과 Q값 평가를 분리\n","* 기존 DQN: 메인 네트워크를 사용하여 행동을 선택하고, 타겟 네트워크를 사용하여 선택된 행동의 Q값을 평가\n","* Double DQN:\n","  * 메인 네트워크를 사용하여 최적 행동 $a^*$ 을 선택\n","  * 타겟 네트워크를 사용하여 선택된 행동 $a^*$ 의 Q값을 평가"],"metadata":{"id":"xpoW6pXCrtlp"}},{"cell_type":"markdown","source":["## [Double DQN 학습 업데이트]\n","\n","* Temporal Difference Tagert (TD Target):\n","  * 기존 DQN: $r + \\gamma \\underset{a'}{max\\;} Q(s', a'; \\theta^-)$\n","  * Double DQN: $r + \\gamma Q(s', \\underset{a}{arg \\text{ } max \\text{ }} Q(s', a; \\theta); \\theta^-)$\n","    * $r$: 즉각적인 보상\n","    * $\\gamma$: 할인율\n","    * $s'$: 다음 상태\n","    * $\\theta$: 메인 네트워크의 파라미터\n","    * $\\theta^-$: 타겟 네트워크의 파라미터\n","    * $\\underset{a}{arg \\text{ } max \\text{ }} Q(s', a; \\theta)$: 메인 네트워크를 사용하여 Q값이 최대가 되는 행동 $a$ 선택\n","    * $Q(s', \\underset{a}{arg \\text{ } max \\text{ }} Q(s', a; \\theta); \\theta^-)$: 선택된 행동 $a$에 대한 타겟 네트워크의 Q값 평가\n","* 손실 함수 (Loss Function): TD 에러를 최소화하는 방향으로 학습\n","  * TD 에러: 현재 네트워크 예측과 실제 타겟 간의 차이\n","  * $Error_{TD, DQN} = Q(s, a; \\theta) - (r + \\gamma \\underset{a'}{max} \\text{ } Q(s', a'; \\theta^-))$\n","    * $\\underset{a'}{max} \\text{ } Q(s', a'; \\theta^-)$: 최대 Q값의 선택과 Q값 평가에 동일한 네트워크($\\theta^-$) 사용\n","    * Q값 과대 추정 문제 발생\n","  * $Error_{TD, DDQN} = Q(s, a; \\theta) - (r + \\gamma Q(s', \\underset{a}{arg \\text{ } max \\text{ }} Q(s', a; \\theta); \\theta^-))$\n","    * $\\underset{a}{arg \\text{ } max \\text{ }} Q(S', a'; \\theta)$: 메인 네트워크($\\theta$)를 사용하여 다음 상태($S'$)에서 가장 높은 Q값을 가지는 행동($a'$)을 선택\n","    * $Q(S', \\underset{a}{arg \\text{ } max \\text{ }} Q(S', a'; \\theta); \\theta')$: 선택된 행동($a'$)의 Q값을 타겟 네트워크($θ'$)를 사용해 평가\n","    * 이 차이가 Q값 과대 추정 문제를 완화하는 데 중요한 역할을 함\n","* 업데이트: 손실 함수를 최소화하는 방향으로 메인 네트워크의 파라미터를 업데이트\n","\n","\n","\n"],"metadata":{"id":"Qz9VzGN9rwTn"}},{"cell_type":"markdown","source":["## [실습: Double DQN을 이용한 CartPole 학습]"],"metadata":{"id":"mR6UTWG9zfX8"}},{"cell_type":"markdown","source":["### 1. 환경 설정\n","\n","* `gym`: CartPole 환경 제공\n","* `torch`: 딥러닝 모델 및 연산 처리\n","* `numpy`: 배열 및 수치 연산\n","* `collections.deque`: 경험 리플레이 버퍼 구현"],"metadata":{"id":"WNEdakvZzqZs"}},{"cell_type":"code","source":["import gymnasium as gym\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from collections import deque\n","\n","env = gym.make('CartPole-v1', render_mode = 'rgb_array')\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n"],"outputs":[],"execution_count":1,"metadata":{"id":"DLvIRoaLzfX-","executionInfo":{"status":"ok","timestamp":1741084835818,"user_tz":-540,"elapsed":10229,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 2. DQN 모델 정의\n","\n","* `DQNAgent` 클래스: 신경망 모델 정의\n","* 3개의 완전 연결 계층 (`nn.Linear`) 사용\n","* ReLU 활성화 함수 적용 (`torch.relu`)\n","* 입력: 상태, 출력: 각 행동의 Q값"],"metadata":{"id":"NcsVjvstzfX_"}},{"cell_type":"code","source":["class DQNAgent(nn.Module):\n","    def __init__(self, state_size, action_size):\n","        super(DQNAgent, self).__init__()\n","        self.fc1 = nn.Linear(state_size, 128)\n","        self.fc2 = nn.Linear(128, 128)\n","        self.fc3 = nn.Linear(128, action_size)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        return self.fc3(x)"],"outputs":[],"execution_count":2,"metadata":{"id":"Ff1r1-_pzfX_","executionInfo":{"status":"ok","timestamp":1741084848845,"user_tz":-540,"elapsed":11,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 3. 하이퍼파라미터 설정\n","\n","* 학습률 (`learning_rate`), 할인율 (`gamma`), 탐험 확률 (`epsilon`) 등 설정\n","* 미니배치 크기 (`batch_size`), 메모리 크기 (`memory_size`) 설정\n","* 타겟 네트워크 업데이트 주기 (`target_update_frequency`) 설정"],"metadata":{"id":"hm481-1_zfYA"}},{"cell_type":"code","source":["learning_rate = 0.0003\n","gamma = 0.99  # 할인율\n","epsilon = 1.0  # 초기 탐험 확률\n","epsilon_min = 0.01\n","epsilon_decay = 0.995\n","batch_size = 128\n","memory_size = 30000\n","target_update_frequency = 5  # 타겟 네트워크 업데이트 주기"],"outputs":[],"execution_count":10,"metadata":{"id":"PpKXIXIvzfYA","executionInfo":{"status":"ok","timestamp":1741085503417,"user_tz":-540,"elapsed":4,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 4. DQN 에이전트 및 타겟 네트워크 생성\n","\n","* `DQNAgent` 모델 인스턴스 생성 (agent, target_agent)\n","* 타겟 네트워크 초기화 (agent 네트워크의 가중치 복사)\n","* Adam optimizer 및 MSE loss function 설정"],"metadata":{"id":"KmN6mOPtzfYA"}},{"cell_type":"code","source":["agent = DQNAgent(state_size, action_size)\n","target_agent = DQNAgent(state_size, action_size)\n","target_agent.load_state_dict(agent.state_dict())\n","optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n","criterion = nn.MSELoss()"],"outputs":[],"execution_count":4,"metadata":{"id":"9WoPFSe7zfYA","executionInfo":{"status":"ok","timestamp":1741084872970,"user_tz":-540,"elapsed":6493,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 5. 경험 리플레이 버퍼 초기화\n","\n","* `deque`를 사용하여 경험 리플레이 버퍼 (`memory`) 초기화"],"metadata":{"id":"0SpkNpCRzfYA"}},{"cell_type":"code","source":["memory = deque(maxlen=memory_size)"],"outputs":[],"execution_count":5,"metadata":{"id":"1JiN4UTEzfYA","executionInfo":{"status":"ok","timestamp":1741084874145,"user_tz":-540,"elapsed":3,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 6. 학습 함수 (`train`)\n","\n","* 경험 리플레이 버퍼에서 미니배치 샘플링\n","* 상태, 행동, 보상, 다음 상태, 종료 여부 추출\n","* **Double DQN 핵심**: agent 네트워크에서 다음 상태의 최대 Q-값을 가지는 행동 선택, target 네트워크에서 선택된 행동의 Q-값 계산\n","* 예측 Q-값과 타겟 Q-값 비교하여 손실 함수 계산\n","* 역전파 및 가중치 업데이트"],"metadata":{"id":"-4UPvXr3zfYB"}},{"cell_type":"code","source":["def train(batch_size):\n","    minibatch = random.sample(memory, batch_size)\n","    states = torch.tensor(np.array([transition[0] for transition in minibatch]), dtype=torch.float32)\n","    actions = torch.tensor(np.array([transition[1] for transition in minibatch]), dtype=torch.long)\n","    rewards = torch.tensor(np.array([transition[2] for transition in minibatch]), dtype=torch.float32)\n","    next_states = torch.tensor(np.array([transition[3] for transition in minibatch]), dtype=torch.float32)\n","    dones = torch.tensor(np.array([transition[4] for transition in minibatch]), dtype=torch.float32)\n","\n","    # Double DQN 핵심: agent 네트워크에서 다음 상태의 최대 Q-값을 가지는 행동 선택\n","    next_actions = agent(next_states).argmax(1).unsqueeze(1)\n","    # target 네트워크에서 선택된 행동의 Q-값 계산\n","    target_q_values = target_agent(next_states).gather(1, next_actions).squeeze()\n","    targets = rewards + gamma * target_q_values * (1 - dones)\n","\n","    predicted_q_values = agent(states).gather(1, actions.unsqueeze(1)).squeeze()\n","    loss = criterion(predicted_q_values, targets)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()"],"outputs":[],"execution_count":6,"metadata":{"id":"ekMJ9PThzfYB","executionInfo":{"status":"ok","timestamp":1741084887198,"user_tz":-540,"elapsed":4,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}}}},{"cell_type":"markdown","source":["### 7. 학습 루프\n","\n","* `episodes`만큼 에피소드 반복\n","* 각 에피소드마다 환경 초기화 및 상태 설정\n","* ε-탐욕적 정책으로 행동 선택\n","* 환경에서 행동 실행 후 다음 상태, 보상, 종료 여부 얻음\n","* 경험 리플레이 버퍼에 경험 추가\n","* 버퍼 크기가 `batch_size` 이상이면 학습 함수 호출\n","* 타겟 네트워크 업데이트 및 탐험 확률 감소\n","* 에피소드 결과 출력"],"metadata":{"id":"6aSovNuszfYB"}},{"cell_type":"code","source":["# 학습 루프\n","episodes = 100\n","for e in range(episodes):\n","    state, _ = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32)\n","    done = False\n","    total_reward = 0\n","\n","    while not done:\n","        if random.random() < epsilon:\n","            action = env.action_space.sample()\n","        else:\n","            action = agent(state).argmax().item()\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        next_state = torch.tensor(next_state, dtype=torch.float32)\n","        done = terminated or truncated\n","\n","        total_reward += reward\n","        memory.append((state, action, reward, next_state, done))\n","\n","        if len(memory) >= batch_size:\n","            train(batch_size)\n","\n","        state = next_state\n","        if e % target_update_frequency == 0:\n","            target_agent.load_state_dict(agent.state_dict())\n","\n","        if epsilon > epsilon_min:\n","            epsilon *= epsilon_decay\n","\n","    if (e+1) % 20 == 0:\n","        print(f\"Episode {e + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 20/100, Total Reward: 302.0, Epsilon: 0.010\n","Episode 40/100, Total Reward: 133.0, Epsilon: 0.010\n","Episode 60/100, Total Reward: 132.0, Epsilon: 0.010\n","Episode 80/100, Total Reward: 124.0, Epsilon: 0.010\n","Episode 100/100, Total Reward: 138.0, Epsilon: 0.010\n"]}],"execution_count":12,"metadata":{"id":"Xz1zE8UfzfYB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741085672724,"user_tz":-540,"elapsed":155664,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}},"outputId":"c13fe15f-aaa0-4427-9196-c2c5f3df17f0"}},{"cell_type":"markdown","source":["### 8. 학습된 에이전트 검증 (렌더링 with GIF)"],"metadata":{"id":"AkHIhwtpV3Ng"}},{"cell_type":"code","source":["import imageio\n","import os\n","\n","frames = []\n","state, _ = env.reset()\n","state = torch.tensor(state, dtype=torch.float32)\n","done = False\n","for i in range(3000): # 3000 steps simulation\n","  action = agent(state).argmax().item()\n","  next_state, reward, terminated, truncated, _ = env.step(action)\n","  next_state = torch.tensor(next_state, dtype=torch.float32)\n","  state = next_state\n","  done = terminated or truncated\n","  frames.append(env.render())\n","\n","  if done:\n","    break\n","\n","imageio.mimsave('cartpole_simulation_doubleDQN.gif', frames, duration=33) # save to gif file\n","print(\"GIF saved as cartpole_simulation.gif\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fQdsiRmOevyM","executionInfo":{"status":"ok","timestamp":1741085719526,"user_tz":-540,"elapsed":6290,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}},"outputId":"2371da55-e716-4099-f3f8-76f67b2ab1ff"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["GIF saved as cartpole_simulation.gif\n"]}]}]}