{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOd0X7QATsE0jcU+F/r9PJe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Soft Actor-Critic (SAC)\n"],"metadata":{"id":"REtX9YFFhWly"}},{"cell_type":"markdown","source":["## [Soft Actor-Critic (SAC) 알고리즘 개요]\n","* 연속 행동 공간 (Continuous Action Space) 문제를 가정\n","* Off-Policy Actor-Critic 계열\n","  * 메모리 활용한 비 실시간 학습\n","* 최대 엔트로피 강화 학습\n","  * 기대 보상을 최대화 하는 동시에 정책의 엔트로피를 최대화\n","  * 탐험 능력 향상 및 안정적인 학습을 기대\n"],"metadata":{"id":"gdaUlk8mpR9n"}},{"cell_type":"markdown","source":["\n","## [연속 행동 공간 문제]\n","\n","* 이산 행동 공간 (Discrete Action Space) vs. 연속 행동 공간 (Continuous Action Space)\n","  * **이산 행동 공간**\n","    * 행동이 유한 개의 선택지로 구성\n","      * CartPole (막대를 왼쪽/오른쪽으로 움직이기)\n","      * Atari 게임 (조이스틱의 상/하/좌/우, 버튼 누르기 등)\n","    * 기존 Q-learning 개념으로 해결 가능\n","      * 어떤 상태에서든, 가능한 모든 행동에 대한 Q-value를 계산할 수 있기 때문\n","\n","  * **연속 행동 공간**\n","    * 행동이 실수 값으로 표현되며, 가능한 행동의 범위가 연속적\n","      * **자동차 핸들 조작**: 핸들을 돌리는 각도 (-90도 ~ +90도)\n","      * **로봇 관절 제어**: 각 관절의 각도 (0도 ~ 360도)\n","      * **드론 프로펠러 속도 조절**: 각 프로펠러의 회전 속도 (0 RPM ~ 10000 RPM)\n","    *   가능한 모든 행동에 대한 Q-value를 계산하는 것이 불가능\n","    * **Q-learning 계열** 알고리즘을 **직접 적용하기 어려움**\n","\n","* 연속 행동 공간에서의 Q-네트워크\n","  * **예시 상황**: 자율 주행 자동차가 차선을 유지하며 주행 중\n","    * **상태 (State)**: 현재 차선의 중앙으로부터 자동차의 위치, 자동차의 현재 속도, 전방 차량과의 거리 등\n","    * **행동 (Action)**: 핸들을 돌리는 각도 (-90도 ~ +90도)\n","  * Q-learing의 Q- 네트워크\n","    * 상태만 입력을 받은 후 모든 행동 경우의 수에 대해 계산을 해야 함\n","    * 예시 상황에서는, -90도, -89.9도, -89.8도, ..., 89.8도, 89.9도, 90도 등 무수히 많은 가능한 핸들 각도 중에서 어떤 것이 최적인지 판단할 수 없음\n","  * Q-네트워크가 **상태**와 **행동**을 **함께** 입력받는다면?\n","    * Q-네트워크는 아래와 같이 특정 상태-행동 쌍에 대한 Q-value를 평가할 수 있음\n","      * *현재 상태에서 핸들을 10도 돌렸을 때의 가치는 얼마인가?*\n","      * *현재 상태에서 핸들을 -5도 돌렸을 때의 가치는 얼마인가?*\n","* 연속 행동 공간에서의 정책 네트워크(Policy Network)\n","  * *A2C와 같이*, 상태를 입력받아 행동의 확률 분포를 출력\n","    * 이 확률 분포에서 행동(예: 11도)을 샘플링하고, Q-네트워크는 이 샘플링된 행동(11도)과 현재 상태를 함께 입력받아 Q-value를 계산"],"metadata":{"id":"L3MQlUd5hnSV"}},{"cell_type":"markdown","source":["## [Soft Actor-Critic (SAC) 특징]\n"],"metadata":{"id":"jUix489bhq4m"}},{"cell_type":"markdown","source":["### 1. **Gaussian Policy**를 활용한 연속적인 행동 표현 및 샘플링\n","* SAC는 **Gaussian Policy(가우시안 정책)**를 사용하여 연속적인 행동을 표현하고 샘플링\n","  * $a \\sim \\mathbb{N}(\\mu(s), \\sigma(s)^2)$\n","    * $a$: 행동\n","    * $\\mu(s)$: 상태 $s$에 대한 평균\n","    * $\\sigma(s)$: 상태 $s$에 대한 표준편차\n","* Gaussian Policy는 행동을 평균과 표준편차로 나타내어 부드러운 행동 제어가 가능하게 함\n","* **예시**\n","  * 로봇 제어에서 모터의 토크를 정밀하게 제어\n","  * 자율 주행에서 핸들의 각도를 부드럽게 조정\n","* 이산적인 행동 공간에서는 불가능한 **미세한 조정**이 필요한 경우에 특히 유용함"],"metadata":{"id":"vlzE4shfiaSz"}},{"cell_type":"markdown","source":["### 2. 효율적인 **Off-policy** 학습\n","* Actor-Critic의 변형이지만, **Off-policy** 방식(DQN과 같은)으로 과거의 경험을 재사용, 학습 효율성을 높임\n","* 기존 알고리즘 대비 **샘플 효율성**이 높아, 연속적인 행동 공간에서 더 많은 데이터를 필요로 하는 문제를 해결하는 데 도움이 됨\n","  \n","  * **엔트로피 보상 (Entropy Regularization)**\n","    * **엔트로피 보상**을 통해 탐험(Exploration)과 활용(Exploitation)의 균형 강화\n","    * **높은 엔트로피**를 유지하도록 장려하여 다양한 행동을 시도하고, 최적 정책을 찾도록 유도\n","    * 연속적인 행동 공간에서는 탐험할 공간이 넓기 때문에, 엔트로피 보상의 효과가 극대화 됨\n","  \n","  * **온도 파라미터 $\\alpha$ 자동 조정**\n","  * **학습 과정**에서 온도 파라미터를 자동 조정\n","    * 엔트로피 목표보다 엔트로피가 낮으면 알파를 증가시켜 탐험성을 높임\n","    * 높으면 알파를 감소시켜 활용도를 높임\n","  * **학습 효율성**을 극대화하며, 연속적인 행동 공간에서 **균형**을 맞추는 데 중요한 역할"],"metadata":{"id":"9omDpVzljGMW"}},{"cell_type":"markdown","source":["### 3. Q값 과대평가 완화 - 두 개의 Q-네트워크\n","* SAC는 두 개의 Q-네트워크를 사용하여 정책 개선 단계에서 발생할 수 있는 Positive Bias를 완화\n","  * 두 Q-네트워크 결과 중 최소를 택해 Policy 네트워크를 업데이트할 때 사용(advantage 계산)"],"metadata":{"id":"5gTsQtAGDhMx"}},{"cell_type":"markdown","source":["### 4. V-네트워크 - 상태 Q-value 예측\n","* 연속 행동 공간에서 행동의 경우의 수가 무한함\n","  * 상태($s$)의 Q-value 즉 $V(s)$를 계산하는 것은 불가능\n","* Policy 네트워크를 업데이트 하기 위한 advantage 계산에 $V(s)$가 필요\n","  * $A(s, a) = Q(s, a) - V(s)$\n","  * $V(s)$는 계산이 불가능하지만, 대신 이를 추정하는 네트워크를 학습해 advantage 계산에 활용"],"metadata":{"id":"OXYobNbcEBmM"}},{"cell_type":"markdown","source":["## [SAC 네트워크 구조]"],"metadata":{"id":"ULRsJk08xp4X"}},{"cell_type":"markdown","source":["### 1. Q-네트워크 (Q-Network)\n","\n","* **상태**와 **행동**을 입력받아 해당 상태-행동 쌍의 Q값을 출력\n","  *   **입력**: 상태 ($s$)와 행동 ($a$)\n","  *   **은닉층**: ReLU 활성화 함수를 사용하는 여러 개의 은닉층\n","  *   **출력**: 해당 상태-행동 쌍의 Q값\n","\n","\\begin{array}{c}\n","\\boxed{(State, Action)} \\\\\n","\\downarrow \\\\\n","\\boxed{\\text{Input Layer}} \\\\\n","\\downarrow \\\\\n","\\boxed{\\text{Hidden Layer}} \\\\\n","\\downarrow \\\\\n","\\boxed{\\text{Output Layer}} \\\\\n","\\downarrow \\\\\n","\\boxed{Q}\n","\\end{array}\n","\n","* SAC는 안정적인 학습을 위해 두 개의 Q-네트워크 (q1, q2)를 사용\n","\n","### 2. 타겟 Q-네트워크 (Target Q-Network)\n","\n","* 타겟 Q-네트워크는 Q-네트워크와 동일한 구조 (target q1, target q2)\n","* 가중치가 주기적으로 업데이트됩니다 (soft update)\n","* 학습의 안정성을 높이는 역할\n","\n","### 3. 정책 네트워크 (Policy Network)\n","\n","* 정책 네트워크는 상태를 입력받아 행동의 확률 분포 (일반적으로 가우시안 분포의 평균과 표준편차)를 출력\n","  * **입력**: 상태 ($s$)\n","  * **은닉층**: ReLU 활성화 함수를 사용하는 여러 개의 은닉층\n","  * **출력**: 행동의 확률 분포를 나타내는 평균 ($\\mu$)과 표준편차 ($\\sigma$)\n","  \n","\\begin{array}{c}\n","\\boxed{State} \\\\\n","\\downarrow \\\\\n","\\boxed{\\text{Input Layer}} \\\\\n","\\downarrow \\\\\n","\\boxed{\\text{Hidden Layer}} \\\\\n","\\downarrow \\\\\n","\\boxed{\\text{Output Layer}} \\\\\n","\\downarrow \\\\\n","\\boxed{(\\text{Mean} \\;\\mu , \\text{Std}\\;\\sigma)}\n","\\end{array}\n","\n","### 4. V-네트워크 (V-Network)\n","\n","* **상태**를 입력받아 해당 상태의 Q값을 추정\n","  *   **입력**: 상태 ($s$)\n","  *   **은닉층**: ReLU 활성화 함수를 사용하는 여러 개의 은닉층\n","  *   **출력**: 해당 상태의 Q값 추정치\n","\n","\\begin{array}{c}\n","\\boxed{(State)} \\\\\n","\\downarrow \\\\\n","\\boxed{\\text{Input Layer}} \\\\\n","\\downarrow \\\\\n","\\boxed{\\text{Hidden Layer}} \\\\\n","\\downarrow \\\\\n","\\boxed{\\text{Output Layer}} \\\\\n","\\downarrow \\\\\n","\\boxed{Q}\n","\\end{array}"],"metadata":{"id":"piunUdccoZQV"}},{"cell_type":"markdown","source":["## [loss 계산 및 update 과정]\n"],"metadata":{"id":"FlIoWgxJIWQ4"}},{"cell_type":"markdown","source":["### 1. 샘플링 (Sampling)\n","\n","* **Replay Buffer에서 Mini-Batch 추출:**\n","$$(s_t, a_t, r_t, s_{t+1}, done_t) \\sim \\mathsf{D}$$\n","  * $s_t$: 시간 t에서의 상태 (state)\n","  * $a_t$: 시간 t에서의 행동 (action)\n","  * $r_t$: 시간 t에서의 보상 (reward)\n","  * $s_{t+1}$: 시간 t+1에서의 다음 상태 (next state)\n","  * $done_t$: 시간 t에서의 종료 여부 (boolean: 종료=1, 아니면=0)\n","  * $\\mathsf{D}$: Replay buffer"],"metadata":{"id":"ybzpgYStJSOb"}},{"cell_type":"markdown","source":["### 2. Target V-value 계산\n","* **Target V-Network를 이용한 다음 상태의 가치 추정:**\n","$$\\tilde{V}(s_{t+1}) = V_{\\bar{\\psi}}(s_{t+1})$$\n","\n","    *   $V_{\\bar{\\psi}}$: Target V-network (파라미터: $\\bar{\\psi}$)\n","\n"],"metadata":{"id":"YXcShkjKJexM"}},{"cell_type":"markdown","source":["### 3. Target Q-value 계산\n","* **두 Q-Network를 이용한 다음 상태-행동 쌍의 Q-value 계산:**\n","$$\\tilde{Q}_1(s_{t+1}, a_{t+1}) = Q_{\\phi_1}(s_{t+1}, a_{t+1})$$\n","$$\\tilde{Q}_2(s_{t+1}, a_{t+1}) = Q_{\\phi_2}(s_{t+1}, a_{t+1})$$\n","  * $Q_{\\phi_1}$, $Q_{\\phi_2}$: 두 개의 Q-networks (파라미터: $\\phi_1$, $\\phi_2$)\n","\n","* **Clipped Double Q-Learning (최소값 선택):**\n","  $$\\tilde{Q}(s_{t+1}, a_{t+1}) = \\min(\\tilde{Q}_1(s_{t+1}, a_{t+1}), \\tilde{Q}_2(s_{t+1}, a_{t+1}))$$\n","\n","* **최종 Target Q-value 계산:**\n","$$\\hat{Q}(s_t, a_t) = r_t + \\gamma (1 - done_t) (\\tilde{Q}(s_{t+1}, a_{t+1}) - \\alpha \\log \\pi_\\theta(a_{t+1} | s_{t+1}))$$\n","  * $\\gamma$: 할인율 (discount factor)\n","  * $\\alpha$: Entropy regularization coefficient (temperature parameter)"],"metadata":{"id":"3lEd4AedKDAU"}},{"cell_type":"markdown","source":["### 4. Q-networks Loss (Critic Loss)\n","\n","* **각 Q-Network의 Mean Squared Error (MSE) Loss 계산:**\n","$$J_Q(\\phi_i) = \\mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \\sim \\mathsf{D}} \\left[ \\frac{1}{2} (Q_{\\phi_i}(s_t, a_t) - \\hat{Q}(s_t, a_t))^2 \\right], \\quad i = 1, 2$$\n","\n","* **최종 Q-Loss (두 Loss의 합):**\n","$$J_Q(\\phi) = J_Q(\\phi_1) + J_Q(\\phi_2)$$\n"],"metadata":{"id":"UYg0q8qSPZYi"}},{"cell_type":"markdown","source":["### 5. V-network Loss (Critic Loss)\n","\n","* **다음 상태에서 Policy Network를 이용해 행동 관련 값 계산:**\n","    $$\\mu_{\\theta}(s_{t+1}), \\log \\sigma_{\\theta}(s_{t+1}) = \\pi_\\theta(s_{t+1})$$\n","\n","  * $\\pi_\\theta$: Policy network (파라미터: $\\theta$)\n","  * $\\mu_{\\theta}(s_{t+1})$: 다음 상태 $s_{t+1}$에서 policy network가 출력하는 평균 행동\n","  * $\\log \\sigma_{\\theta}(s_{t+1})$: 다음 상태 $s_{t+1}$에서 policy network가 출력하는 행동의 로그 표준편차\n","\n","* **다음 행동 샘플링:**\n","    $$a_{t+1} = f\\tanh(z_{t+1})$$\n","\n","  * $z_{t+1} \\sim \\mathsf{N}(\\mu_{\\theta}(s_{t+1}), \\sigma_{\\theta}(s_{t+1}))$: 정규 분포에서 샘플링된 값\n","  * $\\tanh$: $\\tanh$ 함수로 값을 -1 ~ 1 사이에 매치\n","    * 문제에 맞게 추후 scaling 필요할 수 있음\n","\n","* **샘플링된 다음 행동의 Log Probability 계산**\n","  * source code를 이용해 개념 위주로 설명  \n","```python\n","dist = Normal(mean, std)\n","z = dist.rsample()\n","action = torch.tanh(z)\n","log_prob = (dist.log_prob(z) - torch.log(1 - action.pow(2) + 1e-7)).sum(axis=-1, keepdim=True)\n","```\n","    * `dist = Normal(mean, std)`  \n","      * $z \\sim \\mathsf{N}(\\mu, \\sigma^2)$\n","      * 평균(`mean`)이 $\\mu$이고 표준편차(`std`)가 $\\sigma$인 정규 분포(Normal distribution) 객체 `dist`를 생성\n","    * `z = dist.rsample()`\n","      * `dist` 분포에서 reparameterization trick을 사용하여 샘플 `z`를 생성\n","      * `rsample()` 메서드는 내부적으로 다음과 같이 작동\n","        * $z = \\mu + \\sigma \\cdot \\epsilon$,  where [$\\epsilon \\sim \\mathsf{N}(0, 1)$]\n","    * `action = torch.tanh(z)`\n","      * $a = \\tanh(z)$\n","      * 샘플링된 `z` 값에 hyperbolic tangent 함수(`tanh`)를 적용하여 `action`을 생성     \n","      * `tanh` 함수는 출력을 -1과 1 사이로 제한하는 역할\n","    * `z`에서 `action`으로 변환(`tanh` 적용)했으므로, 확률 밀도도 그에 맞게 조정해야함  \n","      * **`dist.log_prob(z)`:** `z`가 `dist` 분포에서 나올 로그 확률 밀도를 계산\n","        * (pytorch에서 제공)\n","      * **`torch.log(1 - action.pow(2) + 1e-7)`:** `tanh` 함수를 적용했기 때문에 발생하는 확률 밀도 변화를 보정하는 항\n","      * Change of variables 공식에 따르면, 다음과 같은 관계가 성립(증명은 생략)\n","        * $\\log p(a) = \\log p(z) - \\log \\left| \\frac{da}{dz} \\right|$\n","          * $p(a)$:  `action`의 확률 밀도\n","          * $\\frac{da}{dz}$:  `action`을 `z`에 대해 미분한 값 (Jacobian)  \n","          $a = \\tanh(z)$ 이므로, $\\frac{da}{dz} = 1 - \\tanh^2(z) = 1 - a^2$\n","        * $- \\log \\left| \\frac{da}{dz} \\right| = -\\log(1 - a^2)$\n","          * 수치적 안정성을 위해(log(0)을 피하기 위해) 매우 작은 값($\\epsilon \\simeq 1e-7$)을 가산\n","        * $-\\log(1 - a^2 + \\epsilon)$\n","      * **`.sum(axis=-1, keepdim=True)`:** 다변량 정규 분포의 경우, 각 차원의 로그 확률을 더하여 전체 로그 확률을 계산\n","        * $\\sum_{i} \\left[ \\log p(z_i) - \\log(1 - a_i^2 + \\epsilon) \\right]$\n","  * $\\therefore \\log \\sigma_\\theta (a_t|s_t) = \\sum_{i} \\left[ \\log p(z_i) - \\log(1 - a_i^2 + \\epsilon) \\right]$\n","\n","* **Q-value 계산:**\n","$$\\tilde{Q}(s_{t}, a_{t})= \\min(Q_{\\phi_1}(s_t, a_t), Q_{\\phi_2}(s_t, a_t))$$\n","\n","* **Target V-value 계산:**\n","$$\\hat{V}(s_t) = \\tilde{Q}(s_{t}, a_{t}) - \\alpha \\log \\sigma_\\theta (a_t|s_t)$$\n","\n","* **V-Network Loss(MSE) 계산:**\n","$$J_V(\\psi) =  \\mathbb{E}_{s_t \\sim \\mathsf{D}, a_t \\sim \\pi_\\theta} \\left[ \\frac{1}{2} (V_\\psi(s_t) - \\hat{V}(s_t))^2 \\right]$$\n","  * $V_\\psi$: V-network (파라미터: $\\psi$)"],"metadata":{"id":"i2y6tFY9VXeY"}},{"cell_type":"markdown","source":["### 6. Policy Loss (Actor Loss)\n","* **샘플링된 행동의 Log Probability 계산:**\n","  * 앞서 계산한 값 활용\n","\n","* **샘플링된 행동에 대한 Q-value 계산 (Clipped Double Q-Learning):**\n","  * 앞서 계산한 값 활용\n","\n","* **Policy Loss 계산:**\n","$$J_\\pi(\\theta) = \\mathbb{E}_{s_t \\sim \\mathsf{D}, \\epsilon_t \\sim \\mathsf{N}} \\left[ \\alpha \\log \\sigma_\\theta (a_t|s_t) - Adv \\right]$$\n","$$ Adv = \\tilde{Q}(s_{t}, a_{t}) -\n","V_\\psi(s_t) $$\n","  * $Adv$: Advantage 추정값\n","\n"],"metadata":{"id":"-3FPQldAW7wp"}},{"cell_type":"markdown","source":["### 7. Alpha Loss (Temperature Parameter Loss)\n","* **샘플링된 행동의 Log Probability 계산:**\n","  * 앞서 계산한 값 활용\n","* **Alpha Loss 계산:**\n","$$J(\\alpha) = \\mathbb{E}_{a_t \\sim \\pi_\\theta} \\left[ -\\alpha \\log \\pi_\\theta(a_t | s_t) - \\alpha \\bar{\\mathsf{H}} \\right]$$\n","  * $\\bar{\\mathsf{H}}$: 목표 엔트로피 (target entropy)"],"metadata":{"id":"LH0AZziJYtSG"}},{"cell_type":"markdown","source":["### 8. Network Update\n","\n","1. **Q-networks 업데이트:**\n","  * $\\phi_i \\leftarrow \\phi_i - \\lambda_Q \\hat{\\nabla}_{\\phi_i} J_Q(\\phi_i)$ for $i = 1, 2$  ($\\lambda_Q$: Q-network learning rate)\n","\n","2. **V-network 업데이트:**\n","  * $\\psi \\leftarrow \\psi - \\lambda_V \\hat{\\nabla}_{\\psi} J_V(\\psi)$  ($\\lambda_V$: V-network learning rate)\n","\n","3. **(Delayed) Policy network 업데이트:**\n","  * $\\theta \\leftarrow \\theta - \\lambda_\\pi \\hat{\\nabla}_{\\theta} J_\\pi(\\theta)$  ($\\lambda_\\pi$: policy learning rate)\n","4. **Alpha 업데이트**\n","  * $\\alpha \\leftarrow \\alpha - \\lambda_{\\alpha} \\hat{\\nabla}_{\\alpha}J(\\alpha)$ ($\\lambda_\\alpha$: alpha learning rate)\n","\n","5. **Target V-network 업데이트:**\n","  * $\\bar{\\psi} \\leftarrow \\tau \\psi + (1 - \\tau) \\bar{\\psi}$"],"metadata":{"id":"4qash5I7Y0-W"}},{"cell_type":"markdown","source":["## [실습: Soft Actor-Critic (SAC)를 이용한 Pendulum 실습]\n","Reference: https://colab.research.google.com/github/MrSyee/pg-is-all-you-need/blob/master/05.SAC.ipynb\n","\n"],"metadata":{"id":"q3-WUBWx-htS"}},{"cell_type":"markdown","source":["### 1. 라이브러리 가져오기 및 하이퍼파라미터 설정"],"metadata":{"id":"sV1Sce1laQ1q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"e3fCpJfJS7En"},"outputs":[],"source":["from typing import Dict, List, Tuple\n","\n","import gymnasium as gym\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.distributions import Normal\n","from IPython.display import clear_output\n","\n","import imageio\n","\n","# parameters\n","num_frames = 50000\n","plotting_interval = 500 # plot refresh 간격 (frames 단위)\n","memory_size = 100000\n","batch_size = 256\n","initial_random_steps = 10000\n","gamma = 0.99  # 할인율\n","lr_actor = 0.0003 # learning rate\n","lr_critic = 0.0003\n","lr_alpha = 0.0003\n","tau = 0.005 # target soft update rate 0: no-update 1: hard-update\n","policy_update_interval = 2\n","early_stop_threshold = -150 # pendulum-v1 대체로 -200 이상이면 성공으로 분류\n","early_stop_patience = 10\n","render_gif_path = \"pendulum_simulation_sac.gif\""]},{"cell_type":"markdown","source":["### 2. 신경망 모델 정의\n","\n","* PolicyNetwork는 평균/표준편차를 이용해 action을 샘플링한 후 로그확률과 함께 출력\n","* QNetwork는 상태와 행동을 입력받아 Q-value를 출력\n","* Value는 상태를 입력받아 V-value를 출력"],"metadata":{"id":"WQuteV1v-htU"}},{"cell_type":"code","source":["def init_layer_uniform(layer: nn.Linear, init_w: float = 3e-3) -> nn.Linear:\n","    \"\"\"Init uniform parameters on the single layer.\"\"\"\n","    layer.weight.data.uniform_(-init_w, init_w)\n","    layer.bias.data.uniform_(-init_w, init_w)\n","\n","    return layer\n","\n","\n","class PolicyNetwork(nn.Module):\n","    def __init__(self, state_dim, action_dim, hidden_dim=256, log_std_min=-20, log_std_max=2):\n","        super(PolicyNetwork, self).__init__()\n","        self.log_std_min = log_std_min\n","        self.log_std_max = log_std_max\n","\n","        self.linear1 = nn.Linear(state_dim, hidden_dim)\n","        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n","\n","        self.log_std_linear = nn.Linear(hidden_dim, action_dim)\n","        self.log_std_linear = init_layer_uniform(self.log_std_linear)\n","        self.mean_linear = nn.Linear(hidden_dim, action_dim)\n","        self.mean_linear = init_layer_uniform(self.mean_linear)\n","\n","    def forward(self, state):\n","        x = F.relu(self.linear1(state))\n","        x = F.relu(self.linear2(x))\n","\n","        mean = self.mean_linear(x).tanh()\n","        log_std = self.log_std_linear(x).tanh()\n","        log_std = self.log_std_min + 0.5 * (self.log_std_max - self.log_std_min) * (log_std + 1)\n","        std = torch.exp(log_std)\n","\n","        # sampling\n","        dist = Normal(mean, std)\n","        z = dist.rsample()\n","        action = z.tanh()\n","\n","        # Enforce action bounds (for continuous action space) 1e-7: log(0)을 계산하는 상황을 막기 위함\n","        log_prob = (dist.log_prob(z) - torch.log(1 - action.pow(2) + 1e-7)).sum(axis=-1, keepdim=True)\n","\n","        return action, log_prob\n","\n","\n","class QNetwork(nn.Module):\n","    def __init__(self, state_dim, action_dim, hidden_dim=256):\n","        super(QNetwork, self).__init__()\n","        self.linear1 = nn.Linear(state_dim + action_dim, hidden_dim)\n","        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.linear3 = nn.Linear(hidden_dim, 1)\n","        self.linear3 = init_layer_uniform(self.linear3)\n","\n","    def forward(self, state, action):\n","        x = torch.cat((state, action), dim=-1)\n","        x = F.relu(self.linear1(x))\n","        x = F.relu(self.linear2(x))\n","        x = self.linear3(x)\n","\n","        return x\n","\n","\n","class VNetwork(nn.Module):  # Optional: Target V-network\n","    def __init__(self, state_dim, hidden_dim=256):\n","        super(VNetwork, self).__init__()\n","        self.linear1 = nn.Linear(state_dim, hidden_dim)\n","        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.linear3 = nn.Linear(hidden_dim, 1)\n","        self.linear3 = init_layer_uniform(self.linear3)\n","\n","    def forward(self, state):\n","        x = F.relu(self.linear1(state))\n","        x = F.relu(self.linear2(x))\n","        x = self.linear3(x)\n","\n","        return x"],"metadata":{"id":"vb2zA0_0VqEj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Replay Buffer 클래스\n","* 에피소드의 경험을 메모리에 저장하는 기능 수행\n","* 메모리에서 배치 단위로 스텝 경험을 샘플링 하는 기능 수행"],"metadata":{"id":"IE4sxSmyaw8y"}},{"cell_type":"code","source":["class ReplayBuffer:\n","    \"\"\"A simple numpy replay buffer.\"\"\"\n","\n","    def __init__(self, obs_dim: int, act_dim: int, size: int, batch_size: int = 32):\n","        \"\"\"Initializate.\"\"\"\n","        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n","        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n","        self.acts_buf = np.zeros([size, act_dim], dtype=np.float32)\n","        self.rews_buf = np.zeros([size], dtype=np.float32)\n","        self.done_buf = np.zeros([size], dtype=np.float32)\n","        self.max_size, self.batch_size = size, batch_size\n","        self.ptr, self.size = 0, 0\n","\n","    def store(\n","        self,\n","        obs: np.ndarray,\n","        act: np.ndarray,\n","        rew: float,\n","        next_obs: np.ndarray,\n","        done: bool,\n","    ):\n","        \"\"\"Store the transition in buffer.\"\"\"\n","        self.obs_buf[self.ptr] = obs\n","        self.next_obs_buf[self.ptr] = next_obs\n","        self.acts_buf[self.ptr] = act\n","        self.rews_buf[self.ptr] = rew\n","        self.done_buf[self.ptr] = done\n","        self.ptr = (self.ptr + 1) % self.max_size\n","        self.size = min(self.size + 1, self.max_size)\n","\n","    def sample_batch(self) -> Dict[str, np.ndarray]:\n","        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n","        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n","        return dict(\n","            obs=self.obs_buf[idxs],\n","            next_obs=self.next_obs_buf[idxs],\n","            acts=self.acts_buf[idxs],\n","            rews=self.rews_buf[idxs],\n","            done=self.done_buf[idxs],\n","        )\n","\n","    def __len__(self) -> int:\n","        return self.size"],"metadata":{"id":"fLdMgwxpbKP4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. SAC Agent 클래스\n","\n","* SAC 알고리즘을 수행하는 에이전트 클래스를 정의\n","* Actor, Critic, Value 네트워크를 초기화\n","* `select_action` 함수는 주어진 상태에서 stochastic action을 선택\n","* `step` 함수는 주어진 행동을 이용해 환경에서 1 step 진행\n","* `update model` 함수는 각종 loss를 계산하고, alpha값과 Actor, Critic, Value 네트워크들을 업데이트\n","* `train` 함수는 정해진 횟수 동안 학습을 진행"],"metadata":{"id":"o82n9-Ey-htU"}},{"cell_type":"code","source":["class SACAgent:\n","    \"\"\"Simplified SAC agent interacting with environment.\"\"\"\n","    def __init__(self, env: gym.Env, seed: int =777):\n","        \"\"\"Initialize.\"\"\"\n","        obs_dim = env.observation_space.shape[0]\n","        action_dim = env.action_space.shape[0]\n","\n","        self.env = env\n","        self.memory = ReplayBuffer(obs_dim, action_dim, memory_size, batch_size)\n","        self.seed = seed\n","\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(self.device)\n","\n","        # Automatic entropy tuning\n","        self.target_entropy = -np.prod((action_dim,)).item()  # 정해진 규칙은 없으나, 널리 사용되는 방식\n","        self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n","        self.alpha_optim = optim.Adam([self.log_alpha], lr=lr_alpha)\n","\n","        # Networks\n","        self.actor = PolicyNetwork(obs_dim, action_dim).to(self.device)\n","        self.v_net = VNetwork(obs_dim).to(self.device)\n","        self.target_v_net = VNetwork(obs_dim).to(self.device)\n","        self.target_v_net.load_state_dict(self.v_net.state_dict())\n","        self.q_net1 = QNetwork(obs_dim, action_dim).to(self.device)\n","        self.q_net2 = QNetwork(obs_dim, action_dim).to(self.device)\n","\n","        # Optimizers\n","        self.actor_optim = optim.Adam(self.actor.parameters(), lr=lr_actor)\n","        self.v_optim = optim.Adam(self.v_net.parameters(), lr=lr_critic)\n","        self.q_optim1 = optim.Adam(self.q_net1.parameters(), lr=lr_critic)\n","        self.q_optim2 = optim.Adam(self.q_net2.parameters(), lr=lr_critic)\n","\n","        # transition to store in memory\n","        self.transition = list()\n","\n","        # total steps count\n","        self.total_step = 0\n","\n","        # mode: train / test\n","        self.is_test = False\n","\n","\n","    def select_action(self, state: np.ndarray) -> np.ndarray:\n","        \"\"\"Select an action from the input state.\"\"\"\n","        if self.total_step < initial_random_steps and not self.is_test:\n","            selected_action = self.env.action_space.sample()\n","        else:\n","            act, _= self.actor(torch.FloatTensor(state).to(self.device))\n","            selected_action = act.detach().cpu().numpy()\n","        self.transition = [state, selected_action]\n","\n","        return selected_action\n","\n","    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool]:\n","        \"\"\"Take an action and return the response of the env.\"\"\"\n","        # network의 action 출력은 -1~1, pendulum의 action space는 -2~2 이므로 이를 scaling\n","        action_input = action * self.env.action_space.high\n","        next_state, reward, terminated, truncated, _ = self.env.step(action_input)\n","        done = terminated or truncated\n","\n","        if not self.is_test:\n","            self.transition += [reward, next_state, done]\n","            self.memory.store(*self.transition)\n","\n","        return next_state, reward, done\n","\n","\n","    def update_model(self) -> None:\n","        \"\"\"Update the model by gradient descent.\"\"\"\n","        samples = self.memory.sample_batch()\n","        state = torch.FloatTensor(samples[\"obs\"]).to(self.device)\n","        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(self.device)\n","        action = torch.FloatTensor(samples[\"acts\"]).to(self.device)\n","        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(self.device)\n","        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(self.device)\n","\n","        new_action, log_prob = self.actor(state)\n","\n","        # train alpha (dual problem)\n","        alpha_loss = (-self.log_alpha.exp() * (log_prob + self.target_entropy).detach()).mean()\n","        self.alpha_optim.zero_grad()\n","        alpha_loss.backward()\n","        self.alpha_optim.step()\n","        alpha = self.log_alpha.exp()  # used for the actor loss calculation\n","\n","        # q_net loss\n","        mask = 1 - done\n","        q1 = self.q_net1(state, action)\n","        q2 = self.q_net2(state, action)\n","        target_v = self.target_v_net(next_state)\n","        target_q = reward + gamma * target_v * mask\n","        q_net1_loss = F.mse_loss(q1, target_q.detach())\n","        q_net2_loss = F.mse_loss(q2, target_q.detach())\n","\n","        # v_net loss\n","        v = self.v_net(state)\n","        q = torch.min(self.q_net1(state, new_action), self.q_net2(state, new_action))\n","        target_v = q - alpha * log_prob\n","        v_loss = F.mse_loss(v, target_v.detach())\n","\n","        if self.total_step % policy_update_interval == 0:\n","            # actor loss\n","            advantage = q - v.detach()\n","            actor_loss = (alpha * log_prob - advantage).mean()\n","\n","            # actor update\n","            self.actor_optim.zero_grad()\n","            actor_loss.backward()\n","            self.actor_optim.step()\n","\n","            # target_v_net update\n","            self._target_soft_update()\n","        else:\n","            actor_loss = torch.zeros(())\n","\n","        # q_net update\n","        self.q_optim1.zero_grad()\n","        q_net1_loss.backward()\n","        self.q_optim1.step()\n","\n","        self.q_optim2.zero_grad()\n","        q_net2_loss.backward()\n","        self.q_optim2.step()\n","\n","        q_loss = q_net1_loss + q_net2_loss\n","\n","        # v_net update\n","        self.v_optim.zero_grad()\n","        v_loss.backward()\n","        self.v_optim.step()\n","\n","\n","    def train(self, num_frames: int = num_frames, plotting_interval: int = 500):\n","        \"\"\"Train the agent.\"\"\"\n","        self.is_test = False\n","\n","        state, _ = self.env.reset(seed=self.seed)\n","        scores = []\n","        score = 0\n","\n","        for self.total_step in range(1, num_frames + 1):\n","            action = self.select_action(state)\n","            next_state, reward, done = self.step(action)\n","\n","            state = next_state  # Update current state\n","            score += reward\n","\n","            if done:\n","                state, _ = self.env.reset(seed=self.seed)\n","                scores.append(score)\n","                score = 0\n","\n","            if len(self.memory) >= batch_size and self.total_step > initial_random_steps:\n","                self.update_model()\n","\n","            if self.total_step % plotting_interval == 0:\n","                self._plot(self.total_step, scores)\n","\n","            # Early stop condition\n","            if self.total_step > initial_random_steps:\n","              if np.min(scores[-early_stop_patience:]) >= early_stop_threshold:\n","                  print(f\"Early stopping triggered! at Step: {self.total_step+1} Score: {score}\")\n","                  break\n","\n","        self.env.close()\n","\n","\n","    def _target_soft_update(self):\n","        \"\"\"Soft-update target network.\"\"\"\n","        for target_param, param in zip(self.target_v_net.parameters(), self.v_net.parameters()):\n","            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n","\n","    def _plot(self, frame_idx: int, scores: List[float],):\n","        \"\"\"Plot the training progresses.\"\"\"\n","        clear_output(wait=True)\n","        plt.figure(figsize=(4, 3))\n","        plt.title(f\"Frame: {frame_idx}, Avg. Score: {np.mean(scores[-10:]):.2f}\")\n","        plt.plot(scores)\n","        plt.show()\n"],"metadata":{"id":"CHoIlgnSesVW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5. 학습 실행 및 결과 시각화\n","\n","*   환경과 에이전트를 생성하고, 학습 루프 함수를 호출합니다.\n","*   학습 결과를 시각화합니다 (평균 reward 그래프)."],"metadata":{"id":"QU-nvk2_-htV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"p00CjqEdS7Ep","executionInfo":{"status":"ok","timestamp":1741077047145,"user_tz":-540,"elapsed":153806,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}},"colab":{"base_uri":"https://localhost:8080/","height":332},"outputId":"646afaba-a99d-4644-b9b4-ae00f0038651"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 400x300 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Early stopping triggered! at Step: 17401 Score: 0\n"]}],"source":["# environment\n","env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n","\n","# 디버깅을 위해 seed 고정 (같은 seed일 때 같은 연산 유도)\n","seed = 777\n","np.random.seed(seed)\n","if torch.backends.cudnn.enabled:\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","agent = SACAgent(env, seed)\n","agent.train(num_frames, plotting_interval)"]},{"cell_type":"markdown","source":["### 6. 학습된 에이전트 검증 및 GIF 저장\n","\n","*   학습된 에이전트를 검증합니다.\n","*   검증 에피소드를 GIF 파일로 저장합니다.\n","*   검증 에피소드의 총 보상을 출력합니다.\n","*   학습이 끝난 후, 이 부분만 별도로 실행합니다."],"metadata":{"id":"gYxZuiLH-htV"}},{"cell_type":"code","source":["state, _ = env.reset(seed = agent.seed)\n","done = False\n","score = 0\n","i = 0\n","frames = []\n","\n","while not done:\n","    i = i + 1\n","    action = agent.select_action(state)\n","    next_state, reward, done = agent.step(action)\n","    state = next_state\n","    score += reward\n","\n","    # 렌더링 이미지를 프레임 리스트에 추가\n","    frames.append(env.render())\n","\n","# GIF 파일로 저장\n","imageio.mimsave(render_gif_path, frames, duration=33)  # fps 조절 가능 duration = 1000/fps\n","print(f\"Test Episode Score: {score}\")\n","\n","env.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"18UZo8oOAs2Y","executionInfo":{"status":"ok","timestamp":1741077205859,"user_tz":-540,"elapsed":9022,"user":{"displayName":"Ki-Baek Lee","userId":"00357310680332321506"}},"outputId":"d857509d-e6f7-4ce7-bd1a-24cca854e23e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Episode Score: -127.97587843816552\n"]}]}]}